{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f03e0a",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f98c2c",
   "metadata": {},
   "source": [
    "Por incompatibilidades entre librerias nos vemos obligados a hacer este workaround para que solucionar los problemas de dependencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43a3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:30: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import  scipy.signal.signaltools\n",
    "import numpy as np\n",
    "\n",
    "def _centered(arr, newsize):\n",
    "    # Return the center newsize portion of the array.\n",
    "    newsize = np.asarray(newsize)\n",
    "    currsize = np.array(arr.shape)\n",
    "    startind = (currsize - newsize) // 2\n",
    "    endind = startind + newsize\n",
    "    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n",
    "    return arr[tuple(myslice)]\n",
    "\n",
    "scipy.signal.signaltools._centered = _centered\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import pickle\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torch\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss, MAPE, MASE\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from  scipy.signal.signaltools import _centered\n",
    "from tft_helper import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # Enables cuDNN auto-tuner for faster runtime when input sizes are consistent\n",
    "\n",
    "basepath = os.path.abspath(\"\")  # script directory\n",
    "\n",
    "sys.path.insert(1, os.path.join(basepath, \"..\\\\\"))\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018de99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install optuna==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5d2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82736cf5",
   "metadata": {},
   "source": [
    "# PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f8001a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "seq_len = 5 #sequence lenght: how many timesteps does a sequence have. For example a week could be considered a single sequence, therefore seq_len would be 5 as as the stock market opens 5 days a week.\n",
    "pred_len = 10 # prediction lenght: How many timesteps does a prediction sequence have. For example if each prediction is a full week pred_len should be 5 as the stock market opens 5 days a week.\n",
    "n_prev_len = 100 # Number of previous timesteps to take for inference. \n",
    "n_preds = 3 # number of predictions with test data\n",
    "test_len = pred_len * n_preds  # Number of timesteps to use for test data.\n",
    "group = \"group\" # If a same model should predict different stores, indices etc specify how to group them. If theres only one time series the set group col to one full of the same value.\n",
    "loss = QuantileLoss() # Loss function. \n",
    "epochs = 75 # Epochs to train the model.\n",
    "\n",
    "# Set ts date range\n",
    "date_start = '2010-01-04' #None #\"2023-06-01\"\n",
    "date_end = \"2024-7-29\"\n",
    "shift = 1 # How many times to shift values. Useful for using last indicator values (RSI, MACD...) for inference\n",
    "ts_indicator_params = {\n",
    "    \"moving_average_windows\": [5, 10, 20, 50, 100, 200], # Moving averages periods\n",
    "    \"sigma_gaussian_filter\": [1,2],\n",
    "    \"n_lags\": 10,\n",
    "                     \n",
    "                     }\n",
    "# cols_to_shift = [2:]\n",
    "\n",
    "# Set training config.\n",
    "lr_finder = False\n",
    "grid_search = \"random\"\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "# logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "# TFT training params.\n",
    "tft_params =  {\"gradient_clip_val\": 0.03, \"hidden_size\": 24, \"dropout\": 0.25, \"hidden_continuous_size\": 24, \"attention_head_size\": 4, \"learning_rate\": 0.01, \"loss\": loss, \"early_stop_callback\": early_stop_callback}\n",
    "\n",
    "\n",
    "PIB_relevant_countries = ['USA',\n",
    " 'CHN',\n",
    " 'EMU',\n",
    " 'DEU',\n",
    " 'FRA',\n",
    " 'GBR',\n",
    " 'JPN',\n",
    " 'IND',\n",
    " 'BRA',\n",
    " 'CAN',\n",
    " 'AUS',\n",
    " 'ITA',\n",
    " 'KOR',\n",
    " 'MEX',\n",
    " 'IDN',\n",
    " 'SAU',\n",
    " 'ZAF',\n",
    " 'TUR',\n",
    " 'ESP']\n",
    "\n",
    "# If grid search, set param grid.\n",
    "# param_grid = {\n",
    "#     \"gradient_clip_val\": [0.01, 0.03, 0.05],\n",
    "#     \"hidden_size\": [8, 16, 32],\n",
    "#     \"dropout\": [0.1, 0.25, 0.4],\n",
    "#     \"hidden_continuous_size\": [8, 16, 32],\n",
    "#     \"attention_head_size\": [2, 4, 8],\n",
    "#     \"learning_rate\": [0.005, 0.01, 0.03],\n",
    "#     \"loss\": [loss],\n",
    "#     \"test_len\": [test_len],\n",
    "#     \"pred_len\": [pred_len],\n",
    "#     \"n_prev_len\": [n_prev_len],\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    \"gradient_clip_val\": [0.01, 0.03, 0.05],\n",
    "    \"hidden_size\": [8, 16, 32],\n",
    "    \"dropout\": [0.1, 0.2, 0.3],\n",
    "    \"hidden_continuous_size\": [16, 64, 128],\n",
    "    \"attention_head_size\": [4,8,16],\n",
    "    \"learning_rate\": [0.005, 0.01, 0.03],\n",
    "    \"loss\": [loss],\n",
    "    \"test_len\": [test_len],\n",
    "    \"pred_len\": [pred_len],\n",
    "    \"n_prev_len\": [n_prev_len],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032f7e1",
   "metadata": {
    "id": "8032f7e1"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf23d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset does not contain volume data.\n",
      "Dataset does not contain volume data.\n"
     ]
    }
   ],
   "source": [
    "# Load time series data.\n",
    "df_nasdaq = load_file(file_name=\"Nasdaq\", path=\"./data/\", ftype=\"csv\")\n",
    "df_nasdaq = investing_preprocessing(df_nasdaq)\n",
    "df_nasdaq = df_nasdaq.rename(columns={\"target\": \"exog_nasdaq\"})\n",
    "\n",
    "df_ibex35 = load_file(file_name=\"Datos hist√≥ricos del IBEX 35\", path=\"./data/\", ftype=\"csv\")\n",
    "df_ibex35 = investing_preprocessing(df_ibex35)\n",
    "df_ibex35 = df_ibex35.rename(columns={\"target\": \"exog_ibex35\"})\n",
    "\n",
    "df_eustoxx = load_file(file_name=\"EUStoxx50\", path=\"./data/\", ftype=\"csv\")\n",
    "df_eustoxx = investing_preprocessing(df_eustoxx)\n",
    "df_eustoxx = df_eustoxx.rename(columns={\"target\": \"exog_eustoxx\"})\n",
    "\n",
    "df_DJ = load_file(file_name=\"DowJones\", path=\"./data/\", ftype=\"csv\")\n",
    "df_DJ = investing_preprocessing(df_DJ)\n",
    "df_DJ = df_DJ.rename(columns={\"target\": \"exog_DJ\"})\n",
    "\n",
    "df_BTC = load_file(file_name=\"BTC\", path=\"./data/\", ftype=\"csv\")\n",
    "df_BTC = investing_preprocessing(df_BTC)\n",
    "df_BTC = df_BTC.rename(columns={\"target\": \"exog_BTC\"})\n",
    "\n",
    "df_syp500 = load_file(file_name=\"S&P500\", path=\"./data/\", ftype=\"csv\")\n",
    "df_syp500 = investing_preprocessing(df_syp500)\n",
    "\n",
    "\n",
    "df = add_ts_as_exog(df_syp500, [df_ibex35, df_nasdaq, df_eustoxx, df_DJ, df_BTC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66dee9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>target</th>\n",
       "      <th>open</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>var</th>\n",
       "      <th>exog_ibex35</th>\n",
       "      <th>exog_nasdaq</th>\n",
       "      <th>exog_eustoxx</th>\n",
       "      <th>exog_DJ</th>\n",
       "      <th>exog_BTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>1133.00</td>\n",
       "      <td>1116.60</td>\n",
       "      <td>1133.90</td>\n",
       "      <td>1116.60</td>\n",
       "      <td>1.61</td>\n",
       "      <td>12145.1</td>\n",
       "      <td>1886.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10583.96</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>1136.50</td>\n",
       "      <td>1132.70</td>\n",
       "      <td>1136.60</td>\n",
       "      <td>1129.70</td>\n",
       "      <td>0.31</td>\n",
       "      <td>12204.4</td>\n",
       "      <td>1888.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10572.02</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>1137.10</td>\n",
       "      <td>1135.70</td>\n",
       "      <td>1139.20</td>\n",
       "      <td>1134.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12222.5</td>\n",
       "      <td>1878.42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10573.68</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>1141.70</td>\n",
       "      <td>1136.30</td>\n",
       "      <td>1142.50</td>\n",
       "      <td>1131.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12166.3</td>\n",
       "      <td>1876.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10606.86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>1145.00</td>\n",
       "      <td>1140.50</td>\n",
       "      <td>1145.40</td>\n",
       "      <td>1136.20</td>\n",
       "      <td>0.29</td>\n",
       "      <td>12163.0</td>\n",
       "      <td>1892.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10618.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>2024-10-21</td>\n",
       "      <td>5853.98</td>\n",
       "      <td>5857.82</td>\n",
       "      <td>5866.92</td>\n",
       "      <td>5824.79</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>11841.1</td>\n",
       "      <td>20361.47</td>\n",
       "      <td>4941.22</td>\n",
       "      <td>42931.60</td>\n",
       "      <td>67371.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>2024-10-22</td>\n",
       "      <td>5851.20</td>\n",
       "      <td>5832.70</td>\n",
       "      <td>5863.04</td>\n",
       "      <td>5821.17</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>11832.7</td>\n",
       "      <td>20383.64</td>\n",
       "      <td>4939.31</td>\n",
       "      <td>42924.89</td>\n",
       "      <td>67427.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>2024-10-23</td>\n",
       "      <td>5797.42</td>\n",
       "      <td>5834.50</td>\n",
       "      <td>5834.85</td>\n",
       "      <td>5762.41</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>11865.2</td>\n",
       "      <td>20066.96</td>\n",
       "      <td>4922.55</td>\n",
       "      <td>42514.95</td>\n",
       "      <td>66663.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>2024-10-24</td>\n",
       "      <td>5809.86</td>\n",
       "      <td>5817.80</td>\n",
       "      <td>5817.80</td>\n",
       "      <td>5784.92</td>\n",
       "      <td>0.21</td>\n",
       "      <td>11839.8</td>\n",
       "      <td>20232.87</td>\n",
       "      <td>4935.45</td>\n",
       "      <td>42374.36</td>\n",
       "      <td>68191.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>2024-10-25</td>\n",
       "      <td>5808.12</td>\n",
       "      <td>5826.75</td>\n",
       "      <td>5862.82</td>\n",
       "      <td>5799.98</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>11812.5</td>\n",
       "      <td>20352.02</td>\n",
       "      <td>4943.09</td>\n",
       "      <td>42114.40</td>\n",
       "      <td>66696.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3729 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   target     open      max      min   var  exog_ibex35  \\\n",
       "0    2010-01-04  1133.00  1116.60  1133.90  1116.60  1.61      12145.1   \n",
       "1    2010-01-05  1136.50  1132.70  1136.60  1129.70  0.31      12204.4   \n",
       "2    2010-01-06  1137.10  1135.70  1139.20  1134.00  0.05      12222.5   \n",
       "3    2010-01-07  1141.70  1136.30  1142.50  1131.30  0.40      12166.3   \n",
       "4    2010-01-08  1145.00  1140.50  1145.40  1136.20  0.29      12163.0   \n",
       "...         ...      ...      ...      ...      ...   ...          ...   \n",
       "3724 2024-10-21  5853.98  5857.82  5866.92  5824.79 -0.18      11841.1   \n",
       "3725 2024-10-22  5851.20  5832.70  5863.04  5821.17 -0.05      11832.7   \n",
       "3726 2024-10-23  5797.42  5834.50  5834.85  5762.41 -0.92      11865.2   \n",
       "3727 2024-10-24  5809.86  5817.80  5817.80  5784.92  0.21      11839.8   \n",
       "3728 2024-10-25  5808.12  5826.75  5862.82  5799.98 -0.03      11812.5   \n",
       "\n",
       "      exog_nasdaq  exog_eustoxx   exog_DJ  exog_BTC  \n",
       "0         1886.70           NaN  10583.96       NaN  \n",
       "1         1888.43           NaN  10572.02       NaN  \n",
       "2         1878.42           NaN  10573.68       NaN  \n",
       "3         1876.72           NaN  10606.86       NaN  \n",
       "4         1892.59           NaN  10618.19       NaN  \n",
       "...           ...           ...       ...       ...  \n",
       "3724     20361.47       4941.22  42931.60   67371.3  \n",
       "3725     20383.64       4939.31  42924.89   67427.3  \n",
       "3726     20066.96       4922.55  42514.95   66663.7  \n",
       "3727     20232.87       4935.45  42374.36   68191.5  \n",
       "3728     20352.02       4943.09  42114.40   66696.8  \n",
       "\n",
       "[3729 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5f849",
   "metadata": {},
   "source": [
    "## ADD INDICATORS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_global_indicators(df, PIB_relevant_countries, date_start, date_end)\n",
    "df = add_indicators(df, ts_indicator_params, categorical_tendency_vars=True)\n",
    "df = df.rename(columns={\"target\": \"exog_syp500\"})\n",
    "df = df.rename(columns={\"target_smoothed_2\": \"target\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8103608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>exog_syp500</th>\n",
       "      <th>open</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>var</th>\n",
       "      <th>exog_ibex35</th>\n",
       "      <th>exog_nasdaq</th>\n",
       "      <th>exog_eustoxx</th>\n",
       "      <th>exog_DJ</th>\n",
       "      <th>...</th>\n",
       "      <th>bullish_rsi</th>\n",
       "      <th>bearish_rsi</th>\n",
       "      <th>bullish_bollinger</th>\n",
       "      <th>bearish_bollinger</th>\n",
       "      <th>bullish_macd</th>\n",
       "      <th>bearish_macd</th>\n",
       "      <th>bullish_atr</th>\n",
       "      <th>bearish_atr</th>\n",
       "      <th>bullish_trend</th>\n",
       "      <th>bearish_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>1133.00</td>\n",
       "      <td>1116.60</td>\n",
       "      <td>1133.90</td>\n",
       "      <td>1116.60</td>\n",
       "      <td>1.61</td>\n",
       "      <td>12145.1</td>\n",
       "      <td>1886.70</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10583.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>1136.50</td>\n",
       "      <td>1132.70</td>\n",
       "      <td>1136.60</td>\n",
       "      <td>1129.70</td>\n",
       "      <td>0.31</td>\n",
       "      <td>12204.4</td>\n",
       "      <td>1888.43</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10572.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>1137.10</td>\n",
       "      <td>1135.70</td>\n",
       "      <td>1139.20</td>\n",
       "      <td>1134.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12222.5</td>\n",
       "      <td>1878.42</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10573.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>1141.70</td>\n",
       "      <td>1136.30</td>\n",
       "      <td>1142.50</td>\n",
       "      <td>1131.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12166.3</td>\n",
       "      <td>1876.72</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10606.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>1145.00</td>\n",
       "      <td>1140.50</td>\n",
       "      <td>1145.40</td>\n",
       "      <td>1136.20</td>\n",
       "      <td>0.29</td>\n",
       "      <td>12163.0</td>\n",
       "      <td>1892.59</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10618.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3661</th>\n",
       "      <td>2024-07-23</td>\n",
       "      <td>5555.74</td>\n",
       "      <td>5565.30</td>\n",
       "      <td>5585.34</td>\n",
       "      <td>5550.90</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>11212.7</td>\n",
       "      <td>19754.34</td>\n",
       "      <td>4916.80</td>\n",
       "      <td>40358.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3662</th>\n",
       "      <td>2024-07-24</td>\n",
       "      <td>5427.13</td>\n",
       "      <td>5505.84</td>\n",
       "      <td>5508.04</td>\n",
       "      <td>5419.98</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>11210.1</td>\n",
       "      <td>19032.39</td>\n",
       "      <td>4861.87</td>\n",
       "      <td>39853.87</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3663</th>\n",
       "      <td>2024-07-25</td>\n",
       "      <td>5399.22</td>\n",
       "      <td>5428.70</td>\n",
       "      <td>5491.59</td>\n",
       "      <td>5390.95</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>11145.6</td>\n",
       "      <td>18830.59</td>\n",
       "      <td>4811.28</td>\n",
       "      <td>39935.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3664</th>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>5459.10</td>\n",
       "      <td>5433.67</td>\n",
       "      <td>5488.32</td>\n",
       "      <td>5430.70</td>\n",
       "      <td>1.11</td>\n",
       "      <td>11165.9</td>\n",
       "      <td>19023.66</td>\n",
       "      <td>4862.50</td>\n",
       "      <td>40589.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3665</th>\n",
       "      <td>2024-07-29</td>\n",
       "      <td>5463.54</td>\n",
       "      <td>5476.55</td>\n",
       "      <td>5487.74</td>\n",
       "      <td>5444.44</td>\n",
       "      <td>0.08</td>\n",
       "      <td>11117.8</td>\n",
       "      <td>19059.49</td>\n",
       "      <td>4815.39</td>\n",
       "      <td>40539.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3666 rows √ó 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  exog_syp500     open      max      min   var  exog_ibex35  \\\n",
       "0    2010-01-04      1133.00  1116.60  1133.90  1116.60  1.61      12145.1   \n",
       "1    2010-01-05      1136.50  1132.70  1136.60  1129.70  0.31      12204.4   \n",
       "2    2010-01-06      1137.10  1135.70  1139.20  1134.00  0.05      12222.5   \n",
       "3    2010-01-07      1141.70  1136.30  1142.50  1131.30  0.40      12166.3   \n",
       "4    2010-01-08      1145.00  1140.50  1145.40  1136.20  0.29      12163.0   \n",
       "...         ...          ...      ...      ...      ...   ...          ...   \n",
       "3661 2024-07-23      5555.74  5565.30  5585.34  5550.90 -0.16      11212.7   \n",
       "3662 2024-07-24      5427.13  5505.84  5508.04  5419.98 -2.31      11210.1   \n",
       "3663 2024-07-25      5399.22  5428.70  5491.59  5390.95 -0.51      11145.6   \n",
       "3664 2024-07-26      5459.10  5433.67  5488.32  5430.70  1.11      11165.9   \n",
       "3665 2024-07-29      5463.54  5476.55  5487.74  5444.44  0.08      11117.8   \n",
       "\n",
       "      exog_nasdaq  exog_eustoxx   exog_DJ  ...  bullish_rsi  bearish_rsi  \\\n",
       "0         1886.70       2324.48  10583.96  ...            0            0   \n",
       "1         1888.43       2324.48  10572.02  ...            0            0   \n",
       "2         1878.42       2324.48  10573.68  ...            0            0   \n",
       "3         1876.72       2324.48  10606.86  ...            0            0   \n",
       "4         1892.59       2324.48  10618.19  ...            0            0   \n",
       "...           ...           ...       ...  ...          ...          ...   \n",
       "3661     19754.34       4916.80  40358.09  ...            0            0   \n",
       "3662     19032.39       4861.87  39853.87  ...            0            0   \n",
       "3663     18830.59       4811.28  39935.07  ...            0            0   \n",
       "3664     19023.66       4862.50  40589.34  ...            0            0   \n",
       "3665     19059.49       4815.39  40539.93  ...            0            0   \n",
       "\n",
       "      bullish_bollinger  bearish_bollinger bullish_macd  bearish_macd  \\\n",
       "0                     0                  0            0             0   \n",
       "1                     0                  0            1             0   \n",
       "2                     0                  0            1             0   \n",
       "3                     0                  0            1             0   \n",
       "4                     0                  0            1             0   \n",
       "...                 ...                ...          ...           ...   \n",
       "3661                  0                  0            0             1   \n",
       "3662                  0                  0            0             1   \n",
       "3663                  1                  0            0             1   \n",
       "3664                  0                  0            0             1   \n",
       "3665                  0                  0            0             1   \n",
       "\n",
       "      bullish_atr  bearish_atr  bullish_trend  bearish_trend  \n",
       "0               0            0              0              0  \n",
       "1               0            0              0              0  \n",
       "2               0            0              0              0  \n",
       "3               0            0              0              0  \n",
       "4               0            0              0              0  \n",
       "...           ...          ...            ...            ...  \n",
       "3661            1            0              0              0  \n",
       "3662            1            0              0              0  \n",
       "3663            1            0              1              0  \n",
       "3664            1            0              0              0  \n",
       "3665            1            0              0              0  \n",
       "\n",
       "[3666 rows x 85 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bef641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHift indicator values\n",
    "cols_to_shift = [col for col in df.columns[3:] if not (col.startswith('AAII') or col.startswith('PIB'))]\n",
    "df[cols_to_shift] = df[cols_to_shift].shift(shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data[\"group\"] = 1\n",
    "\n",
    "# Supongamos que tu DataFrame se llama 'data' y que la columna 'Date' tiene las fechas\n",
    "# Crear una lista de d√≠as festivos (ejemplo, agrega tus d√≠as festivos)\n",
    "dias_festivos = pd.to_datetime([\"2024-01-01\", \"2024-12-25\", ])  # A√±ade m√°s d√≠as festivos\n",
    "\n",
    "# Meses del a√±o (convertir a nombres de meses)\n",
    "data[\"month\"] = data[\"Date\"].dt.strftime('%B')  # Ejemplo: \"January\", \"February\", etc.\n",
    "\n",
    "# D√≠as del a√±o (de 1 a 365 o 366 en a√±os bisiestos), convertir en cadena\n",
    "data[\"day_of_year\"] = data[\"Date\"].dt.dayofyear.astype(str)  # Convertir el n√∫mero de d√≠a a cadena\n",
    "\n",
    "# D√≠as de la semana (de lunes a viernes: 0 = lunes, 4 = viernes), convertir a nombre de d√≠a\n",
    "data[\"weekday\"] = data[\"Date\"].dt.strftime('%A')  # Ejemplo: \"Monday\", \"Tuesday\", etc.\n",
    "\n",
    "# Filtrar para eliminar s√°bados y domingos\n",
    "data = data[data[\"weekday\"].isin([\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"])]\n",
    "\n",
    "# Identificar si el d√≠a es festivo (usar \"Yes\" o \"No\" en lugar de 1 o 0)\n",
    "data[\"is_holiday\"] = data[\"Date\"].isin(dias_festivos).map({True: \"Yes\", False: \"No\"})\n",
    "\n",
    "data[\"time_idx\"] = data.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0651f8e",
   "metadata": {},
   "source": [
    " VARIABLES GROUPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa586286",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest=['Date', 'target',\n",
    "'FEDFUNDS', 'open', 'max', 'min', 'var',\n",
    "'MACD', 'Signal_Line',\n",
    "'group', 'month', 'day_of_year', 'weekday','is_holiday', 'time_idx']\n",
    "PIB_cols = [col for col in df.columns if col.startswith('PIB')]\n",
    "AAII_cols = [col for col in df.columns if col.startswith('AAII')]\n",
    "VIX_cols = [col for col in df.columns if col.endswith('VIX')]\n",
    "SMA_cols = [col for col in df.columns if col.startswith('SMA')]\n",
    "EMA_cols = [col for col in df.columns if col.startswith('EMA')]\n",
    "lag_cols = [col for col in df.columns if col.startswith('target_lag')]\n",
    "target_smoothed_cols = [col for col in df.columns if col.startswith('target_smoothed')]\n",
    "RSI_cols = [col for col in df.columns if col.startswith('RSI')]\n",
    "Bollinger_cols = [col for col in df.columns if col.startswith('Bollinger')]\n",
    "ATR_cols = [col for col in df.columns if col.startswith('ATR')]\n",
    "CCI_cols = [col for col in df.columns if col.startswith('CCI')]\n",
    "ROC_cols = [col for col in df.columns if col.startswith('ROC')]\n",
    "Williams_cols = [col for col in df.columns if col.startswith('Williams')]\n",
    "Stochastic_cols = [col for col in df.columns if col.startswith('Stochastic')]\n",
    "bullish_cols = [col for col in df.columns if col.startswith('bullish')]\n",
    "bearish_cols = [col for col in df.columns if col.startswith('bearish')]\n",
    "exog_ts = [col for col in df.columns if col.startswith('exog')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa86e03",
   "metadata": {},
   "source": [
    "## TRAIN - TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2fd2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "full = data.copy()\n",
    "cols_to_convert = [\"target\", \"open\", \"max\", \"min\", \"var\", \"MACD\", \"Signal_Line\"] + SMA_cols + EMA_cols + Bollinger_cols + ATR_cols + CCI_cols + ROC_cols + Williams_cols + Stochastic_cols + exog_ts + VIX_cols + lag_cols + target_smoothed_cols + RSI_cols\n",
    "full[cols_to_convert] = full[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "full = full.ffill().bfill()\n",
    "\n",
    "data = full[:-test_len]\n",
    "test = full[-test_len:]\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data[\"Date\"][-200:], data[\"target\"][-200:], color=\"blue\", label=\"Train\")\n",
    "plt.plot(test[\"Date\"][-200:], test[\"target\"][-200:], color=\"green\", label=\"Test\")\n",
    "\n",
    "plt.xlabel(\"Fecha y Hora\")\n",
    "plt.ylabel(\"Precio Spot\")\n",
    "plt.title(\"Precio Spot en funci√≥n de Fecha y Hora\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740262e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>exog_syp500</th>\n",
       "      <th>open</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>var</th>\n",
       "      <th>exog_ibex35</th>\n",
       "      <th>exog_nasdaq</th>\n",
       "      <th>exog_eustoxx</th>\n",
       "      <th>exog_DJ</th>\n",
       "      <th>...</th>\n",
       "      <th>bullish_atr</th>\n",
       "      <th>bearish_atr</th>\n",
       "      <th>bullish_trend</th>\n",
       "      <th>bearish_trend</th>\n",
       "      <th>group</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>1133.00</td>\n",
       "      <td>1116.60</td>\n",
       "      <td>1133.90</td>\n",
       "      <td>1116.60</td>\n",
       "      <td>1.61</td>\n",
       "      <td>12145.1</td>\n",
       "      <td>1886.70</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10583.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>4</td>\n",
       "      <td>Monday</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>1136.50</td>\n",
       "      <td>1132.70</td>\n",
       "      <td>1133.90</td>\n",
       "      <td>1116.60</td>\n",
       "      <td>1.61</td>\n",
       "      <td>12145.1</td>\n",
       "      <td>1886.70</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10583.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>5</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>1137.10</td>\n",
       "      <td>1135.70</td>\n",
       "      <td>1136.60</td>\n",
       "      <td>1129.70</td>\n",
       "      <td>0.31</td>\n",
       "      <td>12204.4</td>\n",
       "      <td>1888.43</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10572.02</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>6</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>1141.70</td>\n",
       "      <td>1136.30</td>\n",
       "      <td>1139.20</td>\n",
       "      <td>1134.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>12222.5</td>\n",
       "      <td>1878.42</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10573.68</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>7</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>1145.00</td>\n",
       "      <td>1140.50</td>\n",
       "      <td>1142.50</td>\n",
       "      <td>1131.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>12166.3</td>\n",
       "      <td>1876.72</td>\n",
       "      <td>2324.48</td>\n",
       "      <td>10606.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>8</td>\n",
       "      <td>Friday</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>5346.99</td>\n",
       "      <td>5343.81</td>\n",
       "      <td>5362.35</td>\n",
       "      <td>5335.36</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>11444.0</td>\n",
       "      <td>19021.19</td>\n",
       "      <td>5069.09</td>\n",
       "      <td>38886.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>June</td>\n",
       "      <td>159</td>\n",
       "      <td>Friday</td>\n",
       "      <td>No</td>\n",
       "      <td>3631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3632</th>\n",
       "      <td>2024-06-10</td>\n",
       "      <td>5360.79</td>\n",
       "      <td>5341.22</td>\n",
       "      <td>5375.08</td>\n",
       "      <td>5331.33</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>11404.9</td>\n",
       "      <td>19000.95</td>\n",
       "      <td>5051.31</td>\n",
       "      <td>38798.99</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>June</td>\n",
       "      <td>162</td>\n",
       "      <td>Monday</td>\n",
       "      <td>No</td>\n",
       "      <td>3632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633</th>\n",
       "      <td>2024-06-11</td>\n",
       "      <td>5375.32</td>\n",
       "      <td>5353.00</td>\n",
       "      <td>5365.79</td>\n",
       "      <td>5331.52</td>\n",
       "      <td>0.26</td>\n",
       "      <td>11357.2</td>\n",
       "      <td>19074.68</td>\n",
       "      <td>5016.48</td>\n",
       "      <td>38868.04</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>June</td>\n",
       "      <td>163</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>No</td>\n",
       "      <td>3633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>2024-06-12</td>\n",
       "      <td>5421.03</td>\n",
       "      <td>5409.13</td>\n",
       "      <td>5375.95</td>\n",
       "      <td>5327.25</td>\n",
       "      <td>0.27</td>\n",
       "      <td>11175.5</td>\n",
       "      <td>19210.19</td>\n",
       "      <td>4965.09</td>\n",
       "      <td>38747.42</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>June</td>\n",
       "      <td>164</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>3634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3635</th>\n",
       "      <td>2024-06-13</td>\n",
       "      <td>5433.74</td>\n",
       "      <td>5441.93</td>\n",
       "      <td>5447.25</td>\n",
       "      <td>5409.13</td>\n",
       "      <td>0.85</td>\n",
       "      <td>11245.4</td>\n",
       "      <td>19465.18</td>\n",
       "      <td>5034.43</td>\n",
       "      <td>38712.21</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>June</td>\n",
       "      <td>165</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>No</td>\n",
       "      <td>3635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3636 rows √ó 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  exog_syp500     open      max      min   var  exog_ibex35  \\\n",
       "0    2010-01-04      1133.00  1116.60  1133.90  1116.60  1.61      12145.1   \n",
       "1    2010-01-05      1136.50  1132.70  1133.90  1116.60  1.61      12145.1   \n",
       "2    2010-01-06      1137.10  1135.70  1136.60  1129.70  0.31      12204.4   \n",
       "3    2010-01-07      1141.70  1136.30  1139.20  1134.00  0.05      12222.5   \n",
       "4    2010-01-08      1145.00  1140.50  1142.50  1131.30  0.40      12166.3   \n",
       "...         ...          ...      ...      ...      ...   ...          ...   \n",
       "3631 2024-06-07      5346.99  5343.81  5362.35  5335.36 -0.02      11444.0   \n",
       "3632 2024-06-10      5360.79  5341.22  5375.08  5331.33 -0.11      11404.9   \n",
       "3633 2024-06-11      5375.32  5353.00  5365.79  5331.52  0.26      11357.2   \n",
       "3634 2024-06-12      5421.03  5409.13  5375.95  5327.25  0.27      11175.5   \n",
       "3635 2024-06-13      5433.74  5441.93  5447.25  5409.13  0.85      11245.4   \n",
       "\n",
       "      exog_nasdaq  exog_eustoxx   exog_DJ  ...  bullish_atr  bearish_atr  \\\n",
       "0         1886.70       2324.48  10583.96  ...            0            0   \n",
       "1         1886.70       2324.48  10583.96  ...            0            0   \n",
       "2         1888.43       2324.48  10572.02  ...            0            0   \n",
       "3         1878.42       2324.48  10573.68  ...            0            0   \n",
       "4         1876.72       2324.48  10606.86  ...            0            0   \n",
       "...           ...           ...       ...  ...          ...          ...   \n",
       "3631     19021.19       5069.09  38886.17  ...            0            1   \n",
       "3632     19000.95       5051.31  38798.99  ...            1            0   \n",
       "3633     19074.68       5016.48  38868.04  ...            1            0   \n",
       "3634     19210.19       4965.09  38747.42  ...            1            0   \n",
       "3635     19465.18       5034.43  38712.21  ...            1            0   \n",
       "\n",
       "      bullish_trend  bearish_trend  group    month  day_of_year    weekday  \\\n",
       "0                 0              0      1  January            4     Monday   \n",
       "1                 0              0      1  January            5    Tuesday   \n",
       "2                 0              0      1  January            6  Wednesday   \n",
       "3                 0              0      1  January            7   Thursday   \n",
       "4                 0              0      1  January            8     Friday   \n",
       "...             ...            ...    ...      ...          ...        ...   \n",
       "3631              0              0      1     June          159     Friday   \n",
       "3632              1              0      1     June          162     Monday   \n",
       "3633              1              0      1     June          163    Tuesday   \n",
       "3634              1              0      1     June          164  Wednesday   \n",
       "3635              1              0      1     June          165   Thursday   \n",
       "\n",
       "      is_holiday  time_idx  \n",
       "0             No         0  \n",
       "1             No         1  \n",
       "2             No         2  \n",
       "3             No         3  \n",
       "4             No         4  \n",
       "...          ...       ...  \n",
       "3631          No      3631  \n",
       "3632          No      3632  \n",
       "3633          No      3633  \n",
       "3634          No      3634  \n",
       "3635          No      3635  \n",
       "\n",
       "[3636 rows x 91 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a04371",
   "metadata": {},
   "source": [
    "<!-- ## DF FINL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fc650",
   "metadata": {},
   "source": [
    "## TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdbea69",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "max_prediction_length = pred_len\n",
    "max_encoder_length = n_prev_len \n",
    "# Ajusta training_cutoff para reservar un rango m√°s amplio para la validaci√≥n\n",
    "# validation_size = 50  # ajusta seg√∫n el tama√±o deseado para el conjunto de validaci√≥n\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length #- validation_size\n",
    "\n",
    "train = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target\",\n",
    "    group_ids=[group],\n",
    "    min_encoder_length=max_encoder_length,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    # min_prediction_length=24,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[\"month\", \"weekday\", \"day_of_year\", \"is_holiday\"],  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"Date\", \"time_idx\", 'FEDFUNDS'] + AAII_cols + PIB_cols,\n",
    "                                        \n",
    "    time_varying_unknown_categoricals=  bullish_cols +  bearish_cols,\n",
    "    time_varying_unknown_reals=[\n",
    "        \"target\",\n",
    "        \"open\",\n",
    "        \"max\",\n",
    "        \"min\",\n",
    "        # \"vol\",\n",
    "        \"var\",\n",
    "        \"MACD\",\n",
    "        \"Signal_Line\",\n",
    "    ]     + \n",
    "    lag_cols + \n",
    "    SMA_cols +\n",
    "    EMA_cols +\n",
    "    RSI_cols +\n",
    "    Bollinger_cols +\n",
    "    ATR_cols +\n",
    "    CCI_cols +\n",
    "    ROC_cols +\n",
    "    Stochastic_cols +\n",
    "    Williams_cols +\n",
    "    VIX_cols +\n",
    "    exog_ts +\n",
    "    target_smoothed_cols\n",
    "    ,\n",
    "    # lags={\"target\": list(range(1, 5))},\n",
    "    # target_normalizer=GroupNormalizer(\"standard\", groups=[group], transformation=\"softplus\"),\n",
    "    # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    # add_encoder_length=True,\n",
    "    categorical_encoders={\n",
    "        \"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "        \"weekday\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "        \"day_of_year\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "    },\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(train,  data, predict=True, stop_randomization=True)\n",
    "# test = TimeSeriesDataSet.from_dataset(training, test_data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 32  # set this between 32 to 128\n",
    "train_dataloader = train.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=7, persistent_workers=True\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size, num_workers=7, persistent_workers=True\n",
    ")\n",
    "# test_dataloader = test.to_dataloader(train=False, batch_size=batch_size, num_workers=11, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b716d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "validation_data = data[lambda x: x.time_idx > training_cutoff]\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db5ef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesDataSet[length=1](\n",
       "\ttime_idx='time_idx',\n",
       "\ttarget='target',\n",
       "\tgroup_ids=['group'],\n",
       "\tweight=None,\n",
       "\tmax_encoder_length=100,\n",
       "\tmin_encoder_length=100,\n",
       "\tmin_prediction_idx=0,\n",
       "\tmin_prediction_length=10,\n",
       "\tmax_prediction_length=10,\n",
       "\tstatic_categoricals=[],\n",
       "\tstatic_reals=['target_center', 'target_scale'],\n",
       "\ttime_varying_known_categoricals=['month', 'weekday', 'day_of_year', 'is_holiday'],\n",
       "\ttime_varying_known_reals=['Date', 'time_idx', 'FEDFUNDS', 'AAII_Bullish', 'AAII_Neutral', 'AAII_Bearish', 'PIB_USA', 'PIB_CHN', 'PIB_EMU', 'PIB_DEU', 'PIB_FRA', 'PIB_GBR', 'PIB_JPN', 'PIB_IND', 'PIB_BRA', 'PIB_CAN', 'PIB_AUS', 'PIB_ITA', 'PIB_KOR', 'PIB_MEX', 'PIB_IDN', 'PIB_SAU', 'PIB_ZAF', 'PIB_TUR', 'PIB_ESP', 'relative_time_idx'],\n",
       "\ttime_varying_unknown_categoricals=['bullish_sma_50_200', 'bullish_rsi', 'bullish_bollinger', 'bullish_macd', 'bullish_atr', 'bullish_trend', 'bearish_sma_50_200', 'bearish_rsi', 'bearish_bollinger', 'bearish_macd', 'bearish_atr', 'bearish_trend'],\n",
       "\ttime_varying_unknown_reals=['target', 'open', 'max', 'min', 'var', 'MACD', 'Signal_Line', 'target_lag1', 'target_lag2', 'target_lag3', 'target_lag4', 'target_lag5', 'target_lag6', 'target_lag7', 'target_lag8', 'target_lag9', 'target_lag10', 'SMA_5', 'SMA_10', 'SMA_20', 'SMA_50', 'SMA_100', 'SMA_200', 'EMA_5', 'EMA_10', 'EMA_20', 'EMA_50', 'EMA_100', 'EMA_200', 'RSI_14', 'Bollinger_Upper_20', 'Bollinger_Lower_20', 'ATR_14', 'CCI_10', 'CCI_20', 'ROC_10', 'ROC_14', 'ROC_20', 'Stochastic_14_K', 'Stochastic_14_D', 'Williams_%R_14', 'VIX', 'EUVIX', 'exog_syp500', 'exog_ibex35', 'exog_nasdaq', 'exog_eustoxx', 'exog_DJ', 'target_smoothed_1'],\n",
       "\tvariable_groups={},\n",
       "\tconstant_fill_strategy={},\n",
       "\tallow_missing_timesteps=True,\n",
       "\tlags={},\n",
       "\tadd_relative_time_idx=True,\n",
       "\tadd_target_scales=True,\n",
       "\tadd_encoder_length=False,\n",
       "\ttarget_normalizer=EncoderNormalizer(\n",
       "\tmethod='standard',\n",
       "\tcenter=True,\n",
       "\tmax_length=None,\n",
       "\ttransformation='relu',\n",
       "\tmethod_kwargs={}\n",
       "),\n",
       "\tcategorical_encoders={'month': NaNLabelEncoder(add_nan=True, warn=True), 'weekday': NaNLabelEncoder(add_nan=True, warn=True), 'day_of_year': NaNLabelEncoder(add_nan=True, warn=True), '__group_id__group': NaNLabelEncoder(add_nan=False, warn=True), 'is_holiday': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_sma_50_200': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_rsi': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_bollinger': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_macd': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_atr': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_trend': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_sma_50_200': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_rsi': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_bollinger': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_macd': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_atr': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_trend': NaNLabelEncoder(add_nan=False, warn=True)},\n",
       "\tscalers={'target_center': StandardScaler(), 'target_scale': StandardScaler(), 'Date': StandardScaler(), 'time_idx': StandardScaler(), 'FEDFUNDS': StandardScaler(), 'AAII_Bullish': StandardScaler(), 'AAII_Neutral': StandardScaler(), 'AAII_Bearish': StandardScaler(), 'PIB_USA': StandardScaler(), 'PIB_CHN': StandardScaler(), 'PIB_EMU': StandardScaler(), 'PIB_DEU': StandardScaler(), 'PIB_FRA': StandardScaler(), 'PIB_GBR': StandardScaler(), 'PIB_JPN': StandardScaler(), 'PIB_IND': StandardScaler(), 'PIB_BRA': StandardScaler(), 'PIB_CAN': StandardScaler(), 'PIB_AUS': StandardScaler(), 'PIB_ITA': StandardScaler(), 'PIB_KOR': StandardScaler(), 'PIB_MEX': StandardScaler(), 'PIB_IDN': StandardScaler(), 'PIB_SAU': StandardScaler(), 'PIB_ZAF': StandardScaler(), 'PIB_TUR': StandardScaler(), 'PIB_ESP': StandardScaler(), 'relative_time_idx': StandardScaler(), 'open': StandardScaler(), 'max': StandardScaler(), 'min': StandardScaler(), 'var': StandardScaler(), 'MACD': StandardScaler(), 'Signal_Line': StandardScaler(), 'target_lag1': StandardScaler(), 'target_lag2': StandardScaler(), 'target_lag3': StandardScaler(), 'target_lag4': StandardScaler(), 'target_lag5': StandardScaler(), 'target_lag6': StandardScaler(), 'target_lag7': StandardScaler(), 'target_lag8': StandardScaler(), 'target_lag9': StandardScaler(), 'target_lag10': StandardScaler(), 'SMA_5': StandardScaler(), 'SMA_10': StandardScaler(), 'SMA_20': StandardScaler(), 'SMA_50': StandardScaler(), 'SMA_100': StandardScaler(), 'SMA_200': StandardScaler(), 'EMA_5': StandardScaler(), 'EMA_10': StandardScaler(), 'EMA_20': StandardScaler(), 'EMA_50': StandardScaler(), 'EMA_100': StandardScaler(), 'EMA_200': StandardScaler(), 'RSI_14': StandardScaler(), 'Bollinger_Upper_20': StandardScaler(), 'Bollinger_Lower_20': StandardScaler(), 'ATR_14': StandardScaler(), 'CCI_10': StandardScaler(), 'CCI_20': StandardScaler(), 'ROC_10': StandardScaler(), 'ROC_14': StandardScaler(), 'ROC_20': StandardScaler(), 'Stochastic_14_K': StandardScaler(), 'Stochastic_14_D': StandardScaler(), 'Williams_%R_14': StandardScaler(), 'VIX': StandardScaler(), 'EUVIX': StandardScaler(), 'exog_syp500': StandardScaler(), 'exog_ibex35': StandardScaler(), 'exog_nasdaq': StandardScaler(), 'exog_eustoxx': StandardScaler(), 'exog_DJ': StandardScaler(), 'target_smoothed_1': StandardScaler()},\n",
       "\trandomize_length=None,\n",
       "\tpredict_mode=True\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a24e8",
   "metadata": {},
   "source": [
    "\n",
    "# LEARNIG RATE FINDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa807d",
   "metadata": {},
   "source": [
    "En primer lugar realizamos un estudio para hallar de forma aproximada el valor optimo de tasa de aprendizaje.  No es recomendable al 100% usar el valor sugerido directamente pues a veces no encuentra el mejor, sin embargo si que da un muy buen punto de partida por donde empezar a probar. Para ello usamos un modelo TFT cualquiera basico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54055c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lr_finder:\n",
    "    res = get_best_lr(train, train_dataloader, val_dataloader, **tft_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a1fdd",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927621bc",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ce290",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not grid_search:\n",
    "    # tft_params[\"learning_rate\"] = 0.01\n",
    "    tft, val_loss = tft_trainer(train, train_dataloader, val_dataloader, max_epochs=epochs, **tft_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f0114",
   "metadata": {},
   "source": [
    "### EVAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ae28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not grid_search:\n",
    "    preds = tft_predict(tft, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4ad1d",
   "metadata": {},
   "source": [
    "## GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45763d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------------------------- \n",
      " Probando combinaci√≥n aleatoria 1/100: {'gradient_clip_val': 0.03, 'hidden_size': 32, 'dropout': 0.2, 'hidden_continuous_size': 64, 'attention_head_size': 16, 'learning_rate': 0.005, 'loss': QuantileLoss(quantiles=[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]), 'test_len': 30, 'pred_len': 10, 'n_prev_len': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1023\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1052\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1052\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:178\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:128\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:133\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\fetchers.py:60\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 60\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\utilities\\combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1412\u001b[0m     success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m success:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\connection.py:335\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\connection.py:884\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    882\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 884\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m \u001b[43m_exhaustive_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaithandle_to_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\connection.py:816\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 816\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWaitForMultipleObjects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Llamada a la funci√≥n de b√∫squeda aleatoria\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m best_model, best_params, best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_hyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./plots/gauss_multiexog_syp500_-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn_prev_len\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43md-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_start\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_end\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-lessFilters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/gauss_multiexog_syp500_-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn_prev_len\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43md-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_start\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_end\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\Documents\\Github\\QuantTrader-TFT\\tft_helper.py:381\u001b[0m, in \u001b[0;36mrandom_hyperparameter_search\u001b[1;34m(data, train, train_dataloader, val_dataloader, test, param_grid, n_iterations, max_epochs, save_dir, csv_file)\u001b[0m\n\u001b[0;32m    378\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo con la combinaci√≥n actual de hiperpar√°metros\u001b[39;00m\n\u001b[1;32m--> 381\u001b[0m tft, val_loss \u001b[38;5;241m=\u001b[39m tft_trainer(\n\u001b[0;32m    382\u001b[0m     train, train_dataloader, val_dataloader, max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    383\u001b[0m )\n\u001b[0;32m    385\u001b[0m \u001b[38;5;66;03m# Medir el tiempo de finalizaci√≥n del entrenamiento\u001b[39;00m\n\u001b[0;32m    386\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32mc:\\Users\\nicov\\Documents\\Github\\QuantTrader-TFT\\tft_helper.py:123\u001b[0m, in \u001b[0;36mtft_trainer\u001b[1;34m(train, train_dataloader, val_dataloader, max_epochs, model_path, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     94\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs,\n\u001b[0;32m     95\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m     ],\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    110\u001b[0m tft \u001b[38;5;241m=\u001b[39m TemporalFusionTransformer\u001b[38;5;241m.\u001b[39mfrom_dataset(\n\u001b[0;32m    111\u001b[0m     train,\n\u001b[0;32m    112\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.15\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     reduce_on_plateau_patience\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduce_on_plateau_patience\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m    121\u001b[0m )\n\u001b[1;32m--> 123\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Guardar el modelo entrenado\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "if grid_search == \"random\":\n",
    "    # Suprime todos los warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Llamada a la funci√≥n de b√∫squeda aleatoria\n",
    "    best_model, best_params, best_val_loss = random_hyperparameter_search(\n",
    "        data,\n",
    "        train,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        test,\n",
    "        param_grid,\n",
    "        n_iterations=100,\n",
    "        max_epochs=50,\n",
    "        save_dir=f'./plots/gauss_multiexog_syp500_-{n_prev_len}d-{date_start.replace(\"-\",\"\")}-{date_end.replace(\"-\",\"\")}-lessFilters',\n",
    "        csv_file=f\"./results/gauss_multiexog_syp500_-{n_prev_len}d-{date_start}-{date_end}.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search == \"exhaustive\":\n",
    "    \n",
    "    # Suprime todos los warnings \n",
    "\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Llamada a la funci√≥n de b√∫squeda de hiperpar√°metros\n",
    "    best_model, best_params, best_val_loss = exhaustive_hyperparameter_search(\n",
    "        data, train, train_dataloader, val_dataloader, test, param_grid, max_epochs=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search:\n",
    "    tft_predict(best_model, val_dataloader, n_preds=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03dd31",
   "metadata": {},
   "source": [
    "# TEST EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3272e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(\n",
    "    train,\n",
    "    test,\n",
    "    model,\n",
    "    encoder_lenght,\n",
    "    test_lenght,\n",
    "    pred_lenght,\n",
    "    quantiles: bool = True,\n",
    "):\n",
    "    # group = model.output_transformer.groups[0]\n",
    "    if quantiles:\n",
    "        try:  # for Quantileloss\n",
    "            preds = []\n",
    "            preds_data = pd.concat([train[-encoder_lenght:], test])\n",
    "            for i in range(0, test_lenght, pred_lenght):\n",
    "                new_data = preds_data[i : i + encoder_lenght + pred_lenght]\n",
    "                new_data.loc[:, group] = new_data.iloc[0, new_data.columns.get_loc(group)]\n",
    "                new_raw_predictions = model.predict(new_data, mode=\"raw\", return_x=True)\n",
    "                prediction = []\n",
    "                for i in range(pred_lenght):\n",
    "                    prediction.append(float(new_raw_predictions.output.prediction[0][i][3]))\n",
    "                preds.append(prediction)\n",
    "        except:  # for MQF2DistributionLoss\n",
    "            preds = []\n",
    "            preds_data = pd.concat([train[-encoder_lenght:], test])\n",
    "            for i in range(0, test_lenght, pred_lenght):\n",
    "                new_data = preds_data[i : i + encoder_lenght + pred_lenght]\n",
    "                new_data.loc[:, group] = new_data.iloc[0, new_data.columns.get_loc(group)]\n",
    "                prediction = model.to_prediction(new_raw_predictions.output)[0].flatten().tolist()\n",
    "                preds.append(prediction)\n",
    "    else:\n",
    "        preds = []\n",
    "        preds_data = pd.concat([train[-encoder_lenght:], test])\n",
    "        for i in range(0, test_lenght, pred_lenght):\n",
    "            new_data = preds_data[i : i + encoder_lenght + pred_lenght]\n",
    "            new_data.loc[:, group] = new_data.iloc[0, new_data.columns.get_loc(group)]\n",
    "            new_raw_predictions = model.predict(new_data, mode=\"raw\", return_x=True)\n",
    "            prediction = new_raw_predictions.output.prediction[0].flatten().tolist()\n",
    "            preds.append(prediction)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = []\n",
    "# preds_data = pd.concat([data[-n_prev_len:], test])\n",
    "# # preds_data = preds_data.ffill()\n",
    "# preds_data[\"target\"] = float(1)\n",
    "# # preds_data.drop(columns=[\"target\"], inplace=True)\n",
    "# for i in range(0, test_len, pred_len):\n",
    "#     new_data = preds_data[i : i + n_prev_len + pred_len]\n",
    "#     new_data.loc[:, group] = new_data.iloc[0, new_data.columns.get_loc(group)]\n",
    "#     new_raw_predictions = tft.predict(new_data, mode=\"raw\", return_x=True)\n",
    "#     prediction = new_raw_predictions.output.prediction[0].flatten().tolist()\n",
    "#     preds.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a408112",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = make_preds(\n",
    "    train=data,\n",
    "    test=test,\n",
    "    model=tft,\n",
    "    encoder_lenght=n_prev_len,\n",
    "    test_lenght=test_len,\n",
    "    pred_lenght=pred_len,\n",
    "    quantiles=True if isinstance(loss, QuantileLoss) else False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9847a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, root_mean_squared_error\n",
    "\n",
    "dates = test[\"Date\"].to_list()\n",
    "preds_flat = [item for sublist in preds for item in sublist]\n",
    "\n",
    "# real_vals = list(data[-n_preds * pred_len :][\"target\"])\n",
    "real_vals = test[\"target\"].to_list()\n",
    "\n",
    "print(f\"MAE: {mean_absolute_error(real_vals, preds_flat)}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(real_vals, preds_flat)}\")\n",
    "print(f\"RMSE: {root_mean_squared_error(real_vals, preds_flat)}\")\n",
    "\n",
    "plt.plot(dates, preds_flat, color=\"r\")\n",
    "plt.plot(dates, real_vals, color=\"g\")\n",
    "plt.title(\"Real vs Preds\")\n",
    "plt.xlabel(\"√çndice\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b460c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Convertir las fechas a formato de datetime si no est√°n ya\n",
    "dates = pd.to_datetime(test[\"Date\"]).to_list()\n",
    "\n",
    "# Aplanar las predicciones si es necesario\n",
    "preds_flat = [item for sublist in preds for item in sublist]\n",
    "\n",
    "# Valores reales\n",
    "real_vals = test[\"target\"].to_list()\n",
    "\n",
    "# M√©tricas de error\n",
    "print(f\"MAE: {mean_absolute_error(real_vals, preds_flat)}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(real_vals, preds_flat)}\")\n",
    "print(f\"RMSE: {mean_squared_error(real_vals, preds_flat, squared=False)}\")\n",
    "\n",
    "# Gr√°fico\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates, real_vals, color=\"g\", label=\"Valores Reales\")\n",
    "plt.plot(dates, preds_flat, color=\"r\", label=\"Predicciones\")\n",
    "plt.title(\"Valores Reales vs Predicciones\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Valor (‚Ç¨)\")\n",
    "\n",
    "# Formato de fecha en el eje x\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "plt.gcf().autofmt_xdate()  # Rotar fechas para mejor visualizaci√≥n\n",
    "\n",
    "# A√±adir leyenda\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar gr√°fico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f373476",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Supongamos que `preds`, `test`, y `dates` ya est√°n definidos en tu entorno\n",
    "\n",
    "# Aplanar la lista de predicciones\n",
    "preds_flat = [item for sublist in preds for item in sublist]\n",
    "\n",
    "# Convertir los valores reales a una lista\n",
    "real_vals = test[\"target\"].to_list()\n",
    "\n",
    "# Lista de fechas (timestamps)\n",
    "\n",
    "\n",
    "# Calcular el error absoluto medio\n",
    "print(f\"MAE: {mean_absolute_error(real_vals, preds_flat)}\")\n",
    "print(f\"MAPE: {mean_absolute_percentage_error(real_vals, preds_flat)}\")\n",
    "print(f\"RMSE: {root_mean_squared_error(real_vals, preds_flat)}\")\n",
    "# Crear la gr√°fica\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates, preds_flat, color=\"r\", label=\"Predicciones\", marker=\"o\", linestyle=\"--\")\n",
    "plt.plot(dates, real_vals, color=\"g\", label=\"Valores Reales\", marker=\"x\", linestyle=\"-\")\n",
    "\n",
    "# A√±adir t√≠tulo y etiquetas\n",
    "plt.title(\"Predicciones vs Valores Reales\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Valor\")\n",
    "\n",
    "# Formatear las fechas en el eje x\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n",
    "# plt.gca().xaxis.set_major_locator(mdates.HourLocator(interval=6))\n",
    "plt.gcf().autofmt_xdate()  # Rotar las etiquetas de fecha\n",
    "\n",
    "# A√±adir cuadr√≠cula y leyenda\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar la gr√°fica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d3ac6",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a21f5",
   "metadata": {},
   "source": [
    "## Retrain for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d680c22",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    col for col in data.columns if col != \"target\"\n",
    "]  # Columnas de caracter√≠sticas and col != \"Date\"\n",
    "\n",
    "max_prediction_length = pred_len\n",
    "max_encoder_length = n_prev_len  # 48\n",
    "# training_cutoff = data[\"Date\"].max() - pd.Timedelta(hours=max_encoder_length)\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "retrain = TimeSeriesDataSet(\n",
    "    full.dropna()[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target\",\n",
    "    group_ids=[group],\n",
    "    min_encoder_length=max_encoder_length,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    # min_prediction_length=24,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"Date\", \"month\", \"week\", \"day\", \"time_idx\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"target\",\n",
    "        \"open\",\n",
    "        \"max\",\n",
    "        \"min\",\n",
    "        \"vol\",\n",
    "        \"var\",\n",
    "        \"SMA_5\",\n",
    "        \"EMA_5\",\n",
    "        \"SMA_10\",\n",
    "        \"EMA_10\",\n",
    "        \"SMA_15\",\n",
    "        \"EMA_15\",\n",
    "        \"SMA_20\",\n",
    "        \"EMA_20\",\n",
    "        \"RSI_6\",\n",
    "        \"RSI_10\",\n",
    "        \"RSI_14\",\n",
    "        \"Bollinger_Upper_5\",\n",
    "        \"Bollinger_Lower_5\",\n",
    "        \"Bollinger_Upper_10\",\n",
    "        \"Bollinger_Lower_10\",\n",
    "        \"Bollinger_Upper_15\",\n",
    "        \"Bollinger_Lower_15\",\n",
    "        \"Bollinger_Upper_20\",\n",
    "        \"Bollinger_Lower_20\",\n",
    "        \"MACD\",\n",
    "        \"Signal_Line\",\n",
    "        \"ATR_5\",\n",
    "        \"ATR_10\",\n",
    "        \"ATR_15\",\n",
    "        \"ATR_20\",\n",
    "        \"CCI_5\",\n",
    "        \"CCI_10\",\n",
    "        \"CCI_15\",\n",
    "        \"CCI_20\",\n",
    "        \"ROC_10\",\n",
    "        \"ROC_14\",\n",
    "        \"ROC_20\",\n",
    "        \"ROC_50\",\n",
    "        \"Stochastic_10_K\",\n",
    "        \"Stochastic_10_D\",\n",
    "        \"Stochastic_14_K\",\n",
    "        \"Stochastic_14_D\",\n",
    "        \"Stochastic_20_K\",\n",
    "        \"Stochastic_20_D\",\n",
    "        \"Stochastic_25_K\",\n",
    "        \"Stochastic_25_D\",\n",
    "        \"Stochastic_50_K\",\n",
    "        \"Stochastic_50_D\",\n",
    "        \"Williams_%R_10\",\n",
    "        \"Williams_%R_14\",\n",
    "    ],\n",
    "    # lags={\"target\": list(range(1, 5))},\n",
    "    # target_normalizer=GroupNormalizer(\"standard\", groups=[group], transformation=\"softplus\"),\n",
    "    # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    # add_encoder_length=True,\n",
    "    # categorical_encoders={\n",
    "    #     \"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "    #     \"week\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "    #     \"day\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "    # },\n",
    ")\n",
    "\n",
    "revalidation = TimeSeriesDataSet.from_dataset(retrain, full.dropna(), predict=True, stop_randomization=True)\n",
    "\n",
    "# test = TimeSeriesDataSet.from_dataset(training, test_data, predict=True, stop_randomization=True)\n",
    "\n",
    "\n",
    "# create dataloaders for model\n",
    "\n",
    "batch_size = 64  # set this between 32 to 128\n",
    "\n",
    "retrain_dataloader = retrain.to_dataloader(\n",
    "\n",
    "    train=True, batch_size=batch_size, num_workers=7, persistent_workers=True\n",
    "\n",
    ")\n",
    "\n",
    "reval_dataloader = revalidation.to_dataloader(\n",
    "\n",
    "    train=False, batch_size=batch_size, num_workers=7, persistent_workers=True\n",
    "\n",
    ")\n",
    "\n",
    "# test_dataloader = test.to_dataloader(train=False, batch_size=batch_size, num_workers=11, persistent_workers=True)\n",
    "\n",
    "if not grid_search:\n",
    "\n",
    "    # tft_params[\"learning_rate\"] = 0.01\n",
    "\n",
    "    retft, val_loss = tft_trainer(\n",
    "\n",
    "        retrain, retrain_dataloader, reval_dataloader, max_epochs=epochs, **tft_params\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3c38f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "encoder_data = full[-n_prev_len:]\n",
    "last_row = full.iloc[-1]\n",
    "# Creamos nuevas filas\n",
    "new_rows = []\n",
    "for i in range(1, 6):\n",
    "    new_row = last_row.copy()\n",
    "    new_row[\"Date\"] += timedelta(days=i)\n",
    "    new_row[\"day\"] += i\n",
    "    new_row[\"time_idx\"] += i\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# Concatenamos las nuevas filas al DataFrame original\n",
    "decoder_data = pd.DataFrame(new_rows)\n",
    "\n",
    "new_data = pd.concat([encoder_data, decoder_data], ignore_index=True)\n",
    "new_data.loc[:, group] = new_data.iloc[0, new_data.columns.get_loc(group)]\n",
    "# new_data.loc[-pred_len: ,\"target\"] = 1\n",
    "new_raw_predictions = retft.predict(new_data, mode=\"raw\", return_x=True)\n",
    "if isinstance(loss, QuantileLoss):\n",
    "    prediction = []\n",
    "    for i in range(pred_len):\n",
    "        prediction.append(float(new_raw_predictions.output.prediction[0][i][3]))\n",
    "else:\n",
    "    prediction = new_raw_predictions.output.prediction[0].flatten().tolist()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b0a6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# prediction = [11233.22265625, 11129.837890625, 11228.3486328125, 11315.59375, 11555.85546875]\n",
    "last_vals = full[-10:]\n",
    "last_vals = last_vals[[\"Date\", \"target\"]]\n",
    "fechas_azul = pd.date_range(start=\"2024-08-26\", periods=len(prediction))\n",
    "predictions = pd.DataFrame({\"Date\": fechas_azul, \"target\": prediction})\n",
    "\n",
    "plt.plot(predictions[\"Date\"], predictions[\"target\"], color=\"r\")\n",
    "plt.plot(last_vals[\"Date\"], last_vals[\"target\"], color=\"g\")\n",
    "plt.title(\"Gr√°fica de la lista aplanada\")\n",
    "plt.xlabel(\"√çndice\")\n",
    "plt.ylabel(\"Valor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88078c66",
   "metadata": {},
   "source": [
    "# INTERPRETABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461fd42",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "interpretation = tft.interpret_output(preds.output, reduction=\"sum\") #\n",
    "tft.plot_interpretation(interpretation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735f9d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
