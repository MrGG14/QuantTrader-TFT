{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18f03e0a",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f98c2c",
   "metadata": {},
   "source": [
    "Por incompatibilidades entre librerias nos vemos obligados a hacer este workaround para que solucionar los problemas de dependencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43a3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_forecasting\\models\\base_model.py:30: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import  scipy.signal.signaltools\n",
    "import numpy as np\n",
    "\n",
    "def _centered(arr, newsize):\n",
    "    # Return the center newsize portion of the array.\n",
    "    newsize = np.asarray(newsize)\n",
    "    currsize = np.array(arr.shape)\n",
    "    startind = (currsize - newsize) // 2\n",
    "    endind = startind + newsize\n",
    "    myslice = [slice(startind[k], endind[k]) for k in range(len(endind))]\n",
    "    return arr[tuple(myslice)]\n",
    "\n",
    "scipy.signal.signaltools._centered = _centered\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import pickle\n",
    "import pytorch_forecasting\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import torch\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss, MAPE, MASE\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from  scipy.signal.signaltools import _centered\n",
    "from tft_helper import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "torch.backends.cudnn.benchmark = True # Enables cuDNN auto-tuner for faster runtime when input sizes are consistent\n",
    "\n",
    "basepath = os.path.abspath(\"\")  # script directory\n",
    "\n",
    "sys.path.insert(1, os.path.join(basepath, \"..\\\\\"))\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "018de99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install optuna==3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5d2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82736cf5",
   "metadata": {},
   "source": [
    "# PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b0f9f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_file = 'BTC'\n",
    "exog_files = ['Nasdaq', 'IBEX35', 'EUStoxx50', 'DowJones', 'S&P500', 'USD_EUR', 'GBP_USD', 'USTech100', 'S&P500Futures']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8001a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "seq_len = 5 #sequence lenght: how many timesteps does a sequence have. For example a week could be considered a single sequence, therefore seq_len would be 5 as as the stock market opens 5 days a week.\n",
    "pred_len = 25 # prediction lenght: How many timesteps does a prediction sequence have. For example if each prediction is a full week pred_len should be 5 as the stock market opens 5 days a week.\n",
    "n_prev_len = 50 # Number of previous timesteps to take for inference. \n",
    "n_preds = 4 # number of predictions with test data\n",
    "test_len = pred_len * n_preds  # Number of timesteps to use for test data.\n",
    "group = \"group\" # If a same model should predict different stores, indices etc specify how to group them. If theres only one time series the set group col to one full of the same value.\n",
    "loss = MAE() #QuantileLoss() # Loss function. \n",
    "epochs = 100 # Epochs to train the model.\n",
    "\n",
    "# Set ts date range\n",
    "date_start = '2010-01-04' #None #\"2023-06-01\"\n",
    "date_end = \"2024-7-29\"\n",
    "shift = 1 # How many times to shift values. Useful for using last indicator values (RSI, MACD...) for inference\n",
    "ts_indicator_params = {\n",
    "    \"moving_average_windows\": [5, 10, 20, 50, 100, 200], # Moving averages periods\n",
    "    \"sigma_gaussian_filter\": [1,2],\n",
    "    \"n_lags\": 10,\n",
    "                     \n",
    "                     }\n",
    "# cols_to_shift = [2:]\n",
    "\n",
    "# Set training config.\n",
    "lr_finder = False\n",
    "grid_search = \"random\"\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "# logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "# TFT training params.\n",
    "tft_params =  {\"gradient_clip_val\": 0.03, \"hidden_size\": 24, \"dropout\": 0.25, \"hidden_continuous_size\": 24, \"attention_head_size\": 4, \"learning_rate\": 0.01, \"loss\": loss, \"early_stop_callback\": early_stop_callback}\n",
    "\n",
    "\n",
    "PIB_relevant_countries = ['USA',\n",
    " 'CHN',\n",
    "#  'EMU',\n",
    " 'DEU',\n",
    "#  'FRA',\n",
    "#  'GBR',\n",
    " 'JPN',\n",
    " 'IND',\n",
    " 'BRA',\n",
    " 'CAN',\n",
    "#  'AUS',\n",
    "#  'ITA',\n",
    "#  'KOR',\n",
    "#  'MEX',\n",
    "#  'IDN',\n",
    "#  'SAU',\n",
    "#  'ZAF',\n",
    "#  'TUR',\n",
    "#  'ESP'\n",
    " ]\n",
    "\n",
    "# param_grid = {\n",
    "#     \"gradient_clip_val\": [0.01, 0.03, 0.05],\n",
    "#     \"hidden_size\": [8, 16, 32],\n",
    "#     \"dropout\": [0.1, 0.25, 0.4],\n",
    "#     \"hidden_continuous_size\": [8, 16, 32],\n",
    "#     \"attention_head_size\": [2, 4, 8],\n",
    "#     \"learning_rate\": [0.005, 0.01, 0.03],\n",
    "#     \"loss\": [loss],\n",
    "#     \"test_len\": [test_len],\n",
    "#     \"pred_len\": [pred_len],\n",
    "#     \"n_prev_len\": [n_prev_len],\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    \"gradient_clip_val\": [0.01, 0.03, 0.05],\n",
    "    \"hidden_size\": [8, 16, 32],\n",
    "    \"dropout\": [0.1, 0.2, 0.3],\n",
    "    \"hidden_continuous_size\": [16, 64, 128],\n",
    "    \"attention_head_size\": [4,8,16],\n",
    "    \"learning_rate\": [0.005, 0.01, 0.03],\n",
    "    \"loss\": [loss],\n",
    "    \"test_len\": [test_len],\n",
    "    \"pred_len\": [pred_len],\n",
    "    \"n_prev_len\": [n_prev_len],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032f7e1",
   "metadata": {
    "id": "8032f7e1"
   },
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8621112e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset does not contain volume data.\n",
      "Dataset does not contain volume data.\n",
      "Dataset does not contain volume data.\n",
      "Dataset does not contain volume data.\n",
      "Dataset does not contain volume data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>target</th>\n",
       "      <th>open</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>var</th>\n",
       "      <th>exog_Nasdaq</th>\n",
       "      <th>exog_IBEX35</th>\n",
       "      <th>exog_EUStoxx50</th>\n",
       "      <th>exog_DowJones</th>\n",
       "      <th>exog_S&amp;P500</th>\n",
       "      <th>exog_USD_EUR</th>\n",
       "      <th>exog_GBP_USD</th>\n",
       "      <th>exog_USTech100</th>\n",
       "      <th>exog_S&amp;P500Futures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>434.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>425.9</td>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9208</td>\n",
       "      <td>1.4748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>433.7</td>\n",
       "      <td>434.0</td>\n",
       "      <td>437.4</td>\n",
       "      <td>430.7</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>430.7</td>\n",
       "      <td>433.7</td>\n",
       "      <td>434.1</td>\n",
       "      <td>423.1</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>433.3</td>\n",
       "      <td>430.7</td>\n",
       "      <td>435.3</td>\n",
       "      <td>428.6</td>\n",
       "      <td>0.61</td>\n",
       "      <td>4497.86</td>\n",
       "      <td>9313.2</td>\n",
       "      <td>3164.76</td>\n",
       "      <td>17148.94</td>\n",
       "      <td>2012.7</td>\n",
       "      <td>0.9232</td>\n",
       "      <td>1.4718</td>\n",
       "      <td>4497.9</td>\n",
       "      <td>2009.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>431.2</td>\n",
       "      <td>433.3</td>\n",
       "      <td>435.3</td>\n",
       "      <td>428.9</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>4484.18</td>\n",
       "      <td>9335.2</td>\n",
       "      <td>3178.01</td>\n",
       "      <td>17158.66</td>\n",
       "      <td>2016.7</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>1.4672</td>\n",
       "      <td>4484.2</td>\n",
       "      <td>2011.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>2024-11-20</td>\n",
       "      <td>94303.9</td>\n",
       "      <td>92252.6</td>\n",
       "      <td>94836.1</td>\n",
       "      <td>91517.4</td>\n",
       "      <td>2.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43225.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>1.2650</td>\n",
       "      <td>20664.8</td>\n",
       "      <td>5937.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>2024-11-21</td>\n",
       "      <td>98374.5</td>\n",
       "      <td>94308.7</td>\n",
       "      <td>98937.2</td>\n",
       "      <td>94063.8</td>\n",
       "      <td>4.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9546</td>\n",
       "      <td>1.2586</td>\n",
       "      <td>20695.6</td>\n",
       "      <td>5970.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3248</th>\n",
       "      <td>2024-11-22</td>\n",
       "      <td>98929.7</td>\n",
       "      <td>98381.2</td>\n",
       "      <td>99617.4</td>\n",
       "      <td>97182.2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>1.2531</td>\n",
       "      <td>20800.4</td>\n",
       "      <td>5987.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>2024-11-23</td>\n",
       "      <td>97699.0</td>\n",
       "      <td>98927.2</td>\n",
       "      <td>98927.2</td>\n",
       "      <td>97180.9</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>2024-11-24</td>\n",
       "      <td>96052.7</td>\n",
       "      <td>97696.4</td>\n",
       "      <td>98552.6</td>\n",
       "      <td>95791.4</td>\n",
       "      <td>-1.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20857.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3251 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   target     open      max      min   var  exog_Nasdaq  \\\n",
       "0    2016-01-01    434.0    430.0    438.0    425.9  0.94          NaN   \n",
       "1    2016-01-02    433.7    434.0    437.4    430.7 -0.06          NaN   \n",
       "2    2016-01-03    430.7    433.7    434.1    423.1 -0.70          NaN   \n",
       "3    2016-01-04    433.3    430.7    435.3    428.6  0.61      4497.86   \n",
       "4    2016-01-05    431.2    433.3    435.3    428.9 -0.49      4484.18   \n",
       "...         ...      ...      ...      ...      ...   ...          ...   \n",
       "3246 2024-11-20  94303.9  92252.6  94836.1  91517.4  2.24          NaN   \n",
       "3247 2024-11-21  98374.5  94308.7  98937.2  94063.8  4.32          NaN   \n",
       "3248 2024-11-22  98929.7  98381.2  99617.4  97182.2  0.56          NaN   \n",
       "3249 2024-11-23  97699.0  98927.2  98927.2  97180.9 -1.24          NaN   \n",
       "3250 2024-11-24  96052.7  97696.4  98552.6  95791.4 -1.69          NaN   \n",
       "\n",
       "      exog_IBEX35  exog_EUStoxx50  exog_DowJones  exog_S&P500  exog_USD_EUR  \\\n",
       "0             NaN             NaN            NaN          NaN        0.9208   \n",
       "1             NaN             NaN            NaN          NaN           NaN   \n",
       "2             NaN             NaN            NaN          NaN           NaN   \n",
       "3          9313.2         3164.76       17148.94       2012.7        0.9232   \n",
       "4          9335.2         3178.01       17158.66       2016.7        0.9304   \n",
       "...           ...             ...            ...          ...           ...   \n",
       "3246          NaN             NaN       43225.79          NaN        0.9483   \n",
       "3247          NaN             NaN            NaN          NaN        0.9546   \n",
       "3248          NaN             NaN            NaN          NaN        0.9595   \n",
       "3249          NaN             NaN            NaN          NaN           NaN   \n",
       "3250          NaN             NaN            NaN          NaN           NaN   \n",
       "\n",
       "      exog_GBP_USD  exog_USTech100  exog_S&P500Futures  \n",
       "0           1.4748             NaN                 NaN  \n",
       "1              NaN             NaN                 NaN  \n",
       "2              NaN             NaN                 NaN  \n",
       "3           1.4718          4497.9             2009.00  \n",
       "4           1.4672          4484.2             2011.75  \n",
       "...            ...             ...                 ...  \n",
       "3246        1.2650         20664.8             5937.75  \n",
       "3247        1.2586         20695.6             5970.50  \n",
       "3248        1.2531         20800.4             5987.00  \n",
       "3249           NaN             NaN                 NaN  \n",
       "3250           NaN         20857.4                 NaN  \n",
       "\n",
       "[3251 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_combined_ts_df(target_file, exog_files)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5f849",
   "metadata": {},
   "source": [
    "## ADD INDICATORS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fec0805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicov\\Documents\\Github\\QuantTrader-TFT\\tft_helper.py:1081: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.ffill()\n"
     ]
    }
   ],
   "source": [
    "df = add_global_indicators(df, PIB_relevant_countries, date_start, date_end)\n",
    "df = add_indicators(df, ts_indicator_params, categorical_tendency_vars=True)\n",
    "df = df.rename(columns={\"target\": \"exog_syp500\"})\n",
    "df = df.rename(columns={\"target_smoothed_2\": \"target\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8103608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>exog_syp500</th>\n",
       "      <th>open</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>var</th>\n",
       "      <th>exog_Nasdaq</th>\n",
       "      <th>exog_IBEX35</th>\n",
       "      <th>exog_EUStoxx50</th>\n",
       "      <th>exog_DowJones</th>\n",
       "      <th>...</th>\n",
       "      <th>bullish_rsi</th>\n",
       "      <th>bearish_rsi</th>\n",
       "      <th>bullish_bollinger</th>\n",
       "      <th>bearish_bollinger</th>\n",
       "      <th>bullish_macd</th>\n",
       "      <th>bearish_macd</th>\n",
       "      <th>bullish_atr</th>\n",
       "      <th>bearish_atr</th>\n",
       "      <th>bullish_trend</th>\n",
       "      <th>bearish_trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>434.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>425.9</td>\n",
       "      <td>0.94</td>\n",
       "      <td>4497.86</td>\n",
       "      <td>9313.2</td>\n",
       "      <td>3164.76</td>\n",
       "      <td>17148.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>433.7</td>\n",
       "      <td>434.0</td>\n",
       "      <td>437.4</td>\n",
       "      <td>430.7</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>4497.86</td>\n",
       "      <td>9313.2</td>\n",
       "      <td>3164.76</td>\n",
       "      <td>17148.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>430.7</td>\n",
       "      <td>433.7</td>\n",
       "      <td>434.1</td>\n",
       "      <td>423.1</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>4497.86</td>\n",
       "      <td>9313.2</td>\n",
       "      <td>3164.76</td>\n",
       "      <td>17148.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>433.3</td>\n",
       "      <td>430.7</td>\n",
       "      <td>435.3</td>\n",
       "      <td>428.6</td>\n",
       "      <td>0.61</td>\n",
       "      <td>4497.86</td>\n",
       "      <td>9313.2</td>\n",
       "      <td>3164.76</td>\n",
       "      <td>17148.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>431.2</td>\n",
       "      <td>433.3</td>\n",
       "      <td>435.3</td>\n",
       "      <td>428.9</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>4484.18</td>\n",
       "      <td>9335.2</td>\n",
       "      <td>3178.01</td>\n",
       "      <td>17158.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>2024-07-25</td>\n",
       "      <td>65799.3</td>\n",
       "      <td>65363.9</td>\n",
       "      <td>66088.6</td>\n",
       "      <td>63500.9</td>\n",
       "      <td>0.66</td>\n",
       "      <td>18830.59</td>\n",
       "      <td>11145.6</td>\n",
       "      <td>4811.28</td>\n",
       "      <td>39935.07</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>2024-07-26</td>\n",
       "      <td>67908.6</td>\n",
       "      <td>65799.7</td>\n",
       "      <td>68205.0</td>\n",
       "      <td>65764.3</td>\n",
       "      <td>3.21</td>\n",
       "      <td>19023.66</td>\n",
       "      <td>11165.9</td>\n",
       "      <td>4862.50</td>\n",
       "      <td>40589.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>2024-07-27</td>\n",
       "      <td>67843.1</td>\n",
       "      <td>67910.8</td>\n",
       "      <td>69387.6</td>\n",
       "      <td>66776.8</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>19023.66</td>\n",
       "      <td>11165.9</td>\n",
       "      <td>4862.50</td>\n",
       "      <td>40589.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>2024-07-28</td>\n",
       "      <td>68256.3</td>\n",
       "      <td>67888.9</td>\n",
       "      <td>68291.9</td>\n",
       "      <td>67067.8</td>\n",
       "      <td>0.61</td>\n",
       "      <td>19023.66</td>\n",
       "      <td>11165.9</td>\n",
       "      <td>4862.50</td>\n",
       "      <td>40589.34</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>2024-07-29</td>\n",
       "      <td>66798.7</td>\n",
       "      <td>68256.3</td>\n",
       "      <td>70000.2</td>\n",
       "      <td>66544.5</td>\n",
       "      <td>-2.14</td>\n",
       "      <td>19059.49</td>\n",
       "      <td>11117.8</td>\n",
       "      <td>4815.39</td>\n",
       "      <td>40539.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3133 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  exog_syp500     open      max      min   var  exog_Nasdaq  \\\n",
       "0    2016-01-01        434.0    430.0    438.0    425.9  0.94      4497.86   \n",
       "1    2016-01-02        433.7    434.0    437.4    430.7 -0.06      4497.86   \n",
       "2    2016-01-03        430.7    433.7    434.1    423.1 -0.70      4497.86   \n",
       "3    2016-01-04        433.3    430.7    435.3    428.6  0.61      4497.86   \n",
       "4    2016-01-05        431.2    433.3    435.3    428.9 -0.49      4484.18   \n",
       "...         ...          ...      ...      ...      ...   ...          ...   \n",
       "3128 2024-07-25      65799.3  65363.9  66088.6  63500.9  0.66     18830.59   \n",
       "3129 2024-07-26      67908.6  65799.7  68205.0  65764.3  3.21     19023.66   \n",
       "3130 2024-07-27      67843.1  67910.8  69387.6  66776.8 -0.10     19023.66   \n",
       "3131 2024-07-28      68256.3  67888.9  68291.9  67067.8  0.61     19023.66   \n",
       "3132 2024-07-29      66798.7  68256.3  70000.2  66544.5 -2.14     19059.49   \n",
       "\n",
       "      exog_IBEX35  exog_EUStoxx50  exog_DowJones  ...  bullish_rsi  \\\n",
       "0          9313.2         3164.76       17148.94  ...            0   \n",
       "1          9313.2         3164.76       17148.94  ...            0   \n",
       "2          9313.2         3164.76       17148.94  ...            0   \n",
       "3          9313.2         3164.76       17148.94  ...            0   \n",
       "4          9335.2         3178.01       17158.66  ...            0   \n",
       "...           ...             ...            ...  ...          ...   \n",
       "3128      11145.6         4811.28       39935.07  ...            0   \n",
       "3129      11165.9         4862.50       40589.34  ...            0   \n",
       "3130      11165.9         4862.50       40589.34  ...            0   \n",
       "3131      11165.9         4862.50       40589.34  ...            0   \n",
       "3132      11117.8         4815.39       40539.93  ...            0   \n",
       "\n",
       "      bearish_rsi  bullish_bollinger  bearish_bollinger  bullish_macd  \\\n",
       "0               0                  0                  0             0   \n",
       "1               0                  0                  0             0   \n",
       "2               0                  0                  0             0   \n",
       "3               0                  0                  0             0   \n",
       "4               0                  0                  0             0   \n",
       "...           ...                ...                ...           ...   \n",
       "3128            1                  0                  0             1   \n",
       "3129            1                  0                  0             1   \n",
       "3130            1                  0                  0             1   \n",
       "3131            1                  0                  0             1   \n",
       "3132            0                  0                  0             1   \n",
       "\n",
       "      bearish_macd  bullish_atr  bearish_atr  bullish_trend bearish_trend  \n",
       "0                0            0            0              0             0  \n",
       "1                1            0            0              0             0  \n",
       "2                1            0            0              0             0  \n",
       "3                1            0            0              0             0  \n",
       "4                1            0            0              0             0  \n",
       "...            ...          ...          ...            ...           ...  \n",
       "3128             0            0            1              0             0  \n",
       "3129             0            1            0              1             0  \n",
       "3130             0            0            1              0             0  \n",
       "3131             0            0            1              0             0  \n",
       "3132             0            0            1              0             0  \n",
       "\n",
       "[3133 rows x 78 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2bef641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHift indicator values\n",
    "cols_to_shift = [col for col in df.columns[3:] if not (col.startswith('AAII') or col.startswith('PIB'))]\n",
    "df[cols_to_shift] = df[cols_to_shift].shift(shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179a60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data[\"group\"] = 1\n",
    "\n",
    "# Supongamos que tu DataFrame se llama 'data' y que la columna 'Date' tiene las fechas\n",
    "# Crear una lista de días festivos (ejemplo, agrega tus días festivos)\n",
    "dias_festivos = pd.to_datetime([\"2024-01-01\", \"2024-12-25\", ])  # Añade más días festivos\n",
    "\n",
    "# Meses del año (convertir a nombres de meses)\n",
    "data[\"month\"] = data[\"Date\"].dt.strftime('%B')  # Ejemplo: \"January\", \"February\", etc.\n",
    "\n",
    "# Días del año (de 1 a 365 o 366 en años bisiestos), convertir en cadena\n",
    "data[\"day_of_year\"] = data[\"Date\"].dt.dayofyear.astype(str)  # Convertir el número de día a cadena\n",
    "\n",
    "# Días de la semana (de lunes a viernes: 0 = lunes, 4 = viernes), convertir a nombre de día\n",
    "data[\"weekday\"] = data[\"Date\"].dt.strftime('%A')  # Ejemplo: \"Monday\", \"Tuesday\", etc.\n",
    "\n",
    "# Filtrar para eliminar sábados y domingos\n",
    "data = data[data[\"weekday\"].isin([\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"])]\n",
    "\n",
    "# Identificar si el día es festivo (usar \"Yes\" o \"No\" en lugar de 1 o 0)\n",
    "data[\"is_holiday\"] = data[\"Date\"].isin(dias_festivos).map({True: \"Yes\", False: \"No\"})\n",
    "\n",
    "data[\"time_idx\"] = data.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0651f8e",
   "metadata": {},
   "source": [
    " VARIABLES GROUPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa586286",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest=['Date', 'target',\n",
    "'FEDFUNDS', 'open', 'max', 'min', 'var',\n",
    "'MACD', 'Signal_Line',\n",
    "'group', 'month', 'day_of_year', 'weekday','is_holiday', 'time_idx']\n",
    "PIB_cols = [col for col in df.columns if col.startswith('PIB')]\n",
    "AAII_cols = [col for col in df.columns if col.startswith('AAII')]\n",
    "VIX_cols = [col for col in df.columns if col.endswith('VIX')]\n",
    "SMA_cols = [col for col in df.columns if col.startswith('SMA')]\n",
    "EMA_cols = [col for col in df.columns if col.startswith('EMA')]\n",
    "lag_cols = [col for col in df.columns if col.startswith('target_lag')]\n",
    "target_smoothed_cols = [col for col in df.columns if col.startswith('target_smoothed')]\n",
    "RSI_cols = [col for col in df.columns if col.startswith('RSI')]\n",
    "Bollinger_cols = [col for col in df.columns if col.startswith('Bollinger')]\n",
    "ATR_cols = [col for col in df.columns if col.startswith('ATR')]\n",
    "CCI_cols = [col for col in df.columns if col.startswith('CCI')]\n",
    "ROC_cols = [col for col in df.columns if col.startswith('ROC')]\n",
    "Williams_cols = [col for col in df.columns if col.startswith('Williams')]\n",
    "Stochastic_cols = [col for col in df.columns if col.startswith('Stochastic')]\n",
    "bullish_cols = [col for col in df.columns if col.startswith('bullish')]\n",
    "bearish_cols = [col for col in df.columns if col.startswith('bearish')]\n",
    "exog_ts = [col for col in df.columns if col.startswith('exog')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa86e03",
   "metadata": {},
   "source": [
    "## TRAIN - TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c2fd2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "full = data.copy()\n",
    "cols_to_convert = [\"target\", \"open\", \"max\", \"min\", \"var\", \"MACD\", \"Signal_Line\"] + SMA_cols + EMA_cols + Bollinger_cols + ATR_cols + CCI_cols + ROC_cols + Williams_cols + Stochastic_cols + exog_ts + VIX_cols + lag_cols + target_smoothed_cols + RSI_cols\n",
    "full[cols_to_convert] = full[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "full = full.ffill().bfill()\n",
    "\n",
    "data = full[:-test_len]\n",
    "test = full[-test_len:]\n",
    "data = data.reset_index(drop=True)\n",
    "data = data.dropna()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data[\"Date\"][-200:], data[\"target\"][-200:], color=\"blue\", label=\"Train\")\n",
    "plt.plot(test[\"Date\"][-200:], test[\"target\"][-200:], color=\"green\", label=\"Test\")\n",
    "\n",
    "plt.xlabel(\"Fecha y Hora\")\n",
    "plt.ylabel(\"Precio Spot\")\n",
    "plt.title(\"Precio Spot en función de Fecha y Hora\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "740262e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>exog_syp500</th>\n",
       "      <th>open</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>var</th>\n",
       "      <th>exog_Nasdaq</th>\n",
       "      <th>exog_IBEX35</th>\n",
       "      <th>exog_EUStoxx50</th>\n",
       "      <th>exog_DowJones</th>\n",
       "      <th>...</th>\n",
       "      <th>bullish_atr</th>\n",
       "      <th>bearish_atr</th>\n",
       "      <th>bullish_trend</th>\n",
       "      <th>bearish_trend</th>\n",
       "      <th>group</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>434.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>434.1</td>\n",
       "      <td>423.1</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>4497.86</td>\n",
       "      <td>9313.2</td>\n",
       "      <td>3164.76</td>\n",
       "      <td>17148.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>433.3</td>\n",
       "      <td>430.7</td>\n",
       "      <td>434.1</td>\n",
       "      <td>423.1</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>4497.86</td>\n",
       "      <td>9313.2</td>\n",
       "      <td>3164.76</td>\n",
       "      <td>17148.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>4</td>\n",
       "      <td>Monday</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>431.2</td>\n",
       "      <td>433.3</td>\n",
       "      <td>435.3</td>\n",
       "      <td>428.6</td>\n",
       "      <td>0.61</td>\n",
       "      <td>4497.86</td>\n",
       "      <td>9313.2</td>\n",
       "      <td>3164.76</td>\n",
       "      <td>17148.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>5</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>430.8</td>\n",
       "      <td>431.2</td>\n",
       "      <td>435.3</td>\n",
       "      <td>428.9</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>4484.18</td>\n",
       "      <td>9335.2</td>\n",
       "      <td>3178.01</td>\n",
       "      <td>17158.66</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>6</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>457.0</td>\n",
       "      <td>430.8</td>\n",
       "      <td>432.1</td>\n",
       "      <td>425.0</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>4443.98</td>\n",
       "      <td>9197.4</td>\n",
       "      <td>3139.32</td>\n",
       "      <td>16906.51</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>January</td>\n",
       "      <td>7</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>No</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>2024-03-05</td>\n",
       "      <td>63792.6</td>\n",
       "      <td>68273.1</td>\n",
       "      <td>68495.1</td>\n",
       "      <td>62746.8</td>\n",
       "      <td>8.13</td>\n",
       "      <td>18226.48</td>\n",
       "      <td>10069.8</td>\n",
       "      <td>4912.92</td>\n",
       "      <td>38989.83</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>March</td>\n",
       "      <td>65</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>No</td>\n",
       "      <td>2986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2133</th>\n",
       "      <td>2024-03-06</td>\n",
       "      <td>66080.4</td>\n",
       "      <td>63794.7</td>\n",
       "      <td>69063.1</td>\n",
       "      <td>60138.2</td>\n",
       "      <td>-6.56</td>\n",
       "      <td>17897.87</td>\n",
       "      <td>10117.1</td>\n",
       "      <td>4893.07</td>\n",
       "      <td>38585.19</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>March</td>\n",
       "      <td>66</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>No</td>\n",
       "      <td>2987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134</th>\n",
       "      <td>2024-03-07</td>\n",
       "      <td>66855.3</td>\n",
       "      <td>66074.6</td>\n",
       "      <td>67604.9</td>\n",
       "      <td>62848.7</td>\n",
       "      <td>3.59</td>\n",
       "      <td>18017.57</td>\n",
       "      <td>10197.2</td>\n",
       "      <td>4915.49</td>\n",
       "      <td>38661.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>March</td>\n",
       "      <td>67</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>No</td>\n",
       "      <td>2988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2135</th>\n",
       "      <td>2024-03-08</td>\n",
       "      <td>68172.0</td>\n",
       "      <td>66854.4</td>\n",
       "      <td>67985.5</td>\n",
       "      <td>65602.6</td>\n",
       "      <td>1.17</td>\n",
       "      <td>18297.99</td>\n",
       "      <td>10319.6</td>\n",
       "      <td>4974.22</td>\n",
       "      <td>38791.35</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>March</td>\n",
       "      <td>68</td>\n",
       "      <td>Friday</td>\n",
       "      <td>No</td>\n",
       "      <td>2989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2136</th>\n",
       "      <td>2024-03-11</td>\n",
       "      <td>72099.1</td>\n",
       "      <td>68964.7</td>\n",
       "      <td>69905.3</td>\n",
       "      <td>68165.0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>18018.45</td>\n",
       "      <td>10305.7</td>\n",
       "      <td>4961.11</td>\n",
       "      <td>38722.69</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>March</td>\n",
       "      <td>71</td>\n",
       "      <td>Monday</td>\n",
       "      <td>No</td>\n",
       "      <td>2992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2137 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  exog_syp500     open      max      min   var  exog_Nasdaq  \\\n",
       "0    2016-01-01        434.0    430.0    434.1    423.1 -0.70      4497.86   \n",
       "1    2016-01-04        433.3    430.7    434.1    423.1 -0.70      4497.86   \n",
       "2    2016-01-05        431.2    433.3    435.3    428.6  0.61      4497.86   \n",
       "3    2016-01-06        430.8    431.2    435.3    428.9 -0.49      4484.18   \n",
       "4    2016-01-07        457.0    430.8    432.1    425.0 -0.09      4443.98   \n",
       "...         ...          ...      ...      ...      ...   ...          ...   \n",
       "2132 2024-03-05      63792.6  68273.1  68495.1  62746.8  8.13     18226.48   \n",
       "2133 2024-03-06      66080.4  63794.7  69063.1  60138.2 -6.56     17897.87   \n",
       "2134 2024-03-07      66855.3  66074.6  67604.9  62848.7  3.59     18017.57   \n",
       "2135 2024-03-08      68172.0  66854.4  67985.5  65602.6  1.17     18297.99   \n",
       "2136 2024-03-11      72099.1  68964.7  69905.3  68165.0  0.88     18018.45   \n",
       "\n",
       "      exog_IBEX35  exog_EUStoxx50  exog_DowJones  ...  bullish_atr  \\\n",
       "0          9313.2         3164.76       17148.94  ...            0   \n",
       "1          9313.2         3164.76       17148.94  ...            0   \n",
       "2          9313.2         3164.76       17148.94  ...            0   \n",
       "3          9335.2         3178.01       17158.66  ...            0   \n",
       "4          9197.4         3139.32       16906.51  ...            0   \n",
       "...           ...             ...            ...  ...          ...   \n",
       "2132      10069.8         4912.92       38989.83  ...            1   \n",
       "2133      10117.1         4893.07       38585.19  ...            1   \n",
       "2134      10197.2         4915.49       38661.05  ...            1   \n",
       "2135      10319.6         4974.22       38791.35  ...            1   \n",
       "2136      10305.7         4961.11       38722.69  ...            1   \n",
       "\n",
       "      bearish_atr  bullish_trend  bearish_trend  group    month  day_of_year  \\\n",
       "0               0              0              0      1  January            1   \n",
       "1               0              0              0      1  January            4   \n",
       "2               0              0              0      1  January            5   \n",
       "3               0              0              0      1  January            6   \n",
       "4               0              0              0      1  January            7   \n",
       "...           ...            ...            ...    ...      ...          ...   \n",
       "2132            0              1              0      1    March           65   \n",
       "2133            0              1              0      1    March           66   \n",
       "2134            0              1              0      1    March           67   \n",
       "2135            0              1              0      1    March           68   \n",
       "2136            0              1              0      1    March           71   \n",
       "\n",
       "        weekday  is_holiday  time_idx  \n",
       "0        Friday          No         0  \n",
       "1        Monday          No         3  \n",
       "2       Tuesday          No         4  \n",
       "3     Wednesday          No         5  \n",
       "4      Thursday          No         6  \n",
       "...         ...         ...       ...  \n",
       "2132    Tuesday          No      2986  \n",
       "2133  Wednesday          No      2987  \n",
       "2134   Thursday          No      2988  \n",
       "2135     Friday          No      2989  \n",
       "2136     Monday          No      2992  \n",
       "\n",
       "[2137 rows x 84 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a04371",
   "metadata": {},
   "source": [
    "<!-- ## DF FINL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4fc650",
   "metadata": {},
   "source": [
    "## TimeSeriesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afdbea69",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "max_prediction_length = pred_len\n",
    "max_encoder_length = n_prev_len \n",
    "# Ajusta training_cutoff para reservar un rango más amplio para la validación\n",
    "# validation_size = 50  # ajusta según el tamaño deseado para el conjunto de validación\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length #- validation_size\n",
    "\n",
    "train = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"target\",\n",
    "    group_ids=[group],\n",
    "    min_encoder_length=max_encoder_length,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    # min_prediction_length=24,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[\"month\", \"weekday\", \"day_of_year\", \"is_holiday\"],  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"Date\", \"time_idx\", 'FEDFUNDS'] + AAII_cols + PIB_cols,\n",
    "                                        \n",
    "    time_varying_unknown_categoricals=  bullish_cols +  bearish_cols,\n",
    "    time_varying_unknown_reals=[\n",
    "        \"target\",\n",
    "        \"open\",\n",
    "        \"max\",\n",
    "        \"min\",\n",
    "        # \"vol\",\n",
    "        \"var\",\n",
    "        \"MACD\",\n",
    "        \"Signal_Line\",\n",
    "    ]     + \n",
    "    lag_cols + \n",
    "    SMA_cols +\n",
    "    EMA_cols +\n",
    "    RSI_cols +\n",
    "    Bollinger_cols +\n",
    "    ATR_cols +\n",
    "    CCI_cols +\n",
    "    ROC_cols +\n",
    "    Stochastic_cols +\n",
    "    Williams_cols +\n",
    "    VIX_cols +\n",
    "    exog_ts +\n",
    "    target_smoothed_cols\n",
    "    ,\n",
    "    # lags={\"target\": list(range(1, 5))},\n",
    "    # target_normalizer=GroupNormalizer(\"standard\", groups=[group], transformation=\"softplus\"),\n",
    "    # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    allow_missing_timesteps=True,\n",
    "    # add_encoder_length=True,\n",
    "    categorical_encoders={\n",
    "        \"month\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "        \"weekday\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "        \"day_of_year\": pytorch_forecasting.data.encoders.NaNLabelEncoder(add_nan=True),\n",
    "    },\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(train,  data, predict=True, stop_randomization=True)\n",
    "# test = TimeSeriesDataSet.from_dataset(training, test_data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 32  # set this between 32 to 128\n",
    "train_dataloader = train.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=7, persistent_workers=True\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size, num_workers=7, persistent_workers=True\n",
    ")\n",
    "# test_dataloader = test.to_dataloader(train=False, batch_size=batch_size, num_workers=11, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b716d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "validation_data = data[lambda x: x.time_idx > training_cutoff]\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1db5ef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesDataSet[length=1](\n",
       "\ttime_idx='time_idx',\n",
       "\ttarget='target',\n",
       "\tgroup_ids=['group'],\n",
       "\tweight=None,\n",
       "\tmax_encoder_length=50,\n",
       "\tmin_encoder_length=50,\n",
       "\tmin_prediction_idx=0,\n",
       "\tmin_prediction_length=25,\n",
       "\tmax_prediction_length=25,\n",
       "\tstatic_categoricals=[],\n",
       "\tstatic_reals=['target_center', 'target_scale'],\n",
       "\ttime_varying_known_categoricals=['month', 'weekday', 'day_of_year', 'is_holiday'],\n",
       "\ttime_varying_known_reals=['Date', 'time_idx', 'FEDFUNDS', 'AAII_Bullish', 'AAII_Neutral', 'AAII_Bearish', 'PIB_USA', 'PIB_CHN', 'PIB_DEU', 'PIB_JPN', 'PIB_IND', 'PIB_BRA', 'PIB_CAN', 'relative_time_idx'],\n",
       "\ttime_varying_unknown_categoricals=['bullish_sma_50_200', 'bullish_rsi', 'bullish_bollinger', 'bullish_macd', 'bullish_atr', 'bullish_trend', 'bearish_sma_50_200', 'bearish_rsi', 'bearish_bollinger', 'bearish_macd', 'bearish_atr', 'bearish_trend'],\n",
       "\ttime_varying_unknown_reals=['target', 'open', 'max', 'min', 'var', 'MACD', 'Signal_Line', 'target_lag1', 'target_lag2', 'target_lag3', 'target_lag4', 'target_lag5', 'target_lag6', 'target_lag7', 'target_lag8', 'target_lag9', 'target_lag10', 'SMA_5', 'SMA_10', 'SMA_20', 'SMA_50', 'SMA_100', 'SMA_200', 'EMA_5', 'EMA_10', 'EMA_20', 'EMA_50', 'EMA_100', 'EMA_200', 'RSI_14', 'Bollinger_Upper_20', 'Bollinger_Lower_20', 'ATR_14', 'CCI_10', 'CCI_20', 'ROC_10', 'ROC_14', 'ROC_20', 'Stochastic_14_K', 'Stochastic_14_D', 'Williams_%R_14', 'VIX', 'EUVIX', 'exog_syp500', 'exog_Nasdaq', 'exog_IBEX35', 'exog_EUStoxx50', 'exog_DowJones', 'exog_S&P500', 'exog_USD_EUR', 'exog_GBP_USD', 'exog_USTech100', 'exog_S&P500Futures', 'target_smoothed_1'],\n",
       "\tvariable_groups={},\n",
       "\tconstant_fill_strategy={},\n",
       "\tallow_missing_timesteps=True,\n",
       "\tlags={},\n",
       "\tadd_relative_time_idx=True,\n",
       "\tadd_target_scales=True,\n",
       "\tadd_encoder_length=False,\n",
       "\ttarget_normalizer=EncoderNormalizer(\n",
       "\tmethod='standard',\n",
       "\tcenter=True,\n",
       "\tmax_length=None,\n",
       "\ttransformation='relu',\n",
       "\tmethod_kwargs={}\n",
       "),\n",
       "\tcategorical_encoders={'month': NaNLabelEncoder(add_nan=True, warn=True), 'weekday': NaNLabelEncoder(add_nan=True, warn=True), 'day_of_year': NaNLabelEncoder(add_nan=True, warn=True), '__group_id__group': NaNLabelEncoder(add_nan=False, warn=True), 'is_holiday': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_sma_50_200': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_rsi': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_bollinger': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_macd': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_atr': NaNLabelEncoder(add_nan=False, warn=True), 'bullish_trend': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_sma_50_200': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_rsi': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_bollinger': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_macd': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_atr': NaNLabelEncoder(add_nan=False, warn=True), 'bearish_trend': NaNLabelEncoder(add_nan=False, warn=True)},\n",
       "\tscalers={'target_center': StandardScaler(), 'target_scale': StandardScaler(), 'Date': StandardScaler(), 'time_idx': StandardScaler(), 'FEDFUNDS': StandardScaler(), 'AAII_Bullish': StandardScaler(), 'AAII_Neutral': StandardScaler(), 'AAII_Bearish': StandardScaler(), 'PIB_USA': StandardScaler(), 'PIB_CHN': StandardScaler(), 'PIB_DEU': StandardScaler(), 'PIB_JPN': StandardScaler(), 'PIB_IND': StandardScaler(), 'PIB_BRA': StandardScaler(), 'PIB_CAN': StandardScaler(), 'relative_time_idx': StandardScaler(), 'open': StandardScaler(), 'max': StandardScaler(), 'min': StandardScaler(), 'var': StandardScaler(), 'MACD': StandardScaler(), 'Signal_Line': StandardScaler(), 'target_lag1': StandardScaler(), 'target_lag2': StandardScaler(), 'target_lag3': StandardScaler(), 'target_lag4': StandardScaler(), 'target_lag5': StandardScaler(), 'target_lag6': StandardScaler(), 'target_lag7': StandardScaler(), 'target_lag8': StandardScaler(), 'target_lag9': StandardScaler(), 'target_lag10': StandardScaler(), 'SMA_5': StandardScaler(), 'SMA_10': StandardScaler(), 'SMA_20': StandardScaler(), 'SMA_50': StandardScaler(), 'SMA_100': StandardScaler(), 'SMA_200': StandardScaler(), 'EMA_5': StandardScaler(), 'EMA_10': StandardScaler(), 'EMA_20': StandardScaler(), 'EMA_50': StandardScaler(), 'EMA_100': StandardScaler(), 'EMA_200': StandardScaler(), 'RSI_14': StandardScaler(), 'Bollinger_Upper_20': StandardScaler(), 'Bollinger_Lower_20': StandardScaler(), 'ATR_14': StandardScaler(), 'CCI_10': StandardScaler(), 'CCI_20': StandardScaler(), 'ROC_10': StandardScaler(), 'ROC_14': StandardScaler(), 'ROC_20': StandardScaler(), 'Stochastic_14_K': StandardScaler(), 'Stochastic_14_D': StandardScaler(), 'Williams_%R_14': StandardScaler(), 'VIX': StandardScaler(), 'EUVIX': StandardScaler(), 'exog_syp500': StandardScaler(), 'exog_Nasdaq': StandardScaler(), 'exog_IBEX35': StandardScaler(), 'exog_EUStoxx50': StandardScaler(), 'exog_DowJones': StandardScaler(), 'exog_S&P500': StandardScaler(), 'exog_USD_EUR': StandardScaler(), 'exog_GBP_USD': StandardScaler(), 'exog_USTech100': StandardScaler(), 'exog_S&P500Futures': StandardScaler(), 'target_smoothed_1': StandardScaler()},\n",
       "\trandomize_length=None,\n",
       "\tpredict_mode=True\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2a24e8",
   "metadata": {},
   "source": [
    "\n",
    "# LEARNIG RATE FINDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa807d",
   "metadata": {},
   "source": [
    "En primer lugar realizamos un estudio para hallar de forma aproximada el valor optimo de tasa de aprendizaje.  No es recomendable al 100% usar el valor sugerido directamente pues a veces no encuentra el mejor, sin embargo si que da un muy buen punto de partida por donde empezar a probar. Para ello usamos un modelo TFT cualquiera basico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54055c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if lr_finder:\n",
    "    res = get_best_lr(train, train_dataloader, val_dataloader, **tft_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a1fdd",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927621bc",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "152ce290",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not grid_search:\n",
    "    # tft_params[\"learning_rate\"] = 0.01\n",
    "    tft, val_loss = tft_trainer(train, train_dataloader, val_dataloader, max_epochs=epochs, **tft_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f0114",
   "metadata": {},
   "source": [
    "### EVAL MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba6ae28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not grid_search:\n",
    "    preds = tft_predict(tft, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4ad1d",
   "metadata": {},
   "source": [
    "## GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45763d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -------------------------------------------------------- \n",
      " Probando combinación aleatoria 1/100: {'gradient_clip_val': 0.05, 'hidden_size': 16, 'dropout': 0.1, 'hidden_continuous_size': 16, 'attention_head_size': 4, 'learning_rate': 0.005, 'loss': QuantileLoss(quantiles=[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]), 'test_len': 100, 'pred_len': 25, 'n_prev_len': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:  91%|█████████ | 29/32 [00:13<00:01,  2.14it/s, v_num=16, train_loss_step=1.13e+3, val_loss=7.57e+3, train_loss_epoch=943.0]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    570\u001b[0m     ckpt_path,\n\u001b[0;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m )\n\u001b[1;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:250\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 250\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:190\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:268\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:167\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 167\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\core\\module.py:1306\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \n\u001b[0;32m   1305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1306\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:153\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:238\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:122\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_optimizer\\optimizer\\ranger.py:100\u001b[0m, in \u001b[0;36mRanger.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 100\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:108\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03mhook is called.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:144\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:138\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:239\u001b[0m, in \u001b[0;36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[1;34m(loss)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_fn\u001b[39m(loss: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m     \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:212\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[1;34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mbackward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, optimizer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m closure_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mpost_backward(closure_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:72\u001b[0m, in \u001b[0;36mPrecision.backward\u001b[1;34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m model\u001b[38;5;241m.\u001b[39mbackward(tensor, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\core\\module.py:1101\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[1;34m(self, loss, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1101\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Llamada a la función de búsqueda aleatoria\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m best_model, best_params, best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrandom_hyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./plots/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_gauss_multiexog_-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn_prev_len\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43md-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_start\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_end\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtarget_file\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_gauss_multiexog-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mn_prev_len\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43md-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_start\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdate_end\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\Documents\\Github\\QuantTrader-TFT\\tft_helper.py:333\u001b[0m, in \u001b[0;36mrandom_hyperparameter_search\u001b[1;34m(data, train, train_dataloader, val_dataloader, test, param_grid, n_iterations, max_epochs, save_dir, csv_file)\u001b[0m\n\u001b[0;32m    330\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo con la combinación actual de hiperparámetros\u001b[39;00m\n\u001b[1;32m--> 333\u001b[0m tft, val_loss \u001b[38;5;241m=\u001b[39m tft_trainer(\n\u001b[0;32m    334\u001b[0m     train, train_dataloader, val_dataloader, max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    335\u001b[0m )\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# Medir el tiempo de finalización del entrenamiento\u001b[39;00m\n\u001b[0;32m    338\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n",
      "File \u001b[1;32mc:\\Users\\nicov\\Documents\\Github\\QuantTrader-TFT\\tft_helper.py:123\u001b[0m, in \u001b[0;36mtft_trainer\u001b[1;34m(train, train_dataloader, val_dataloader, max_epochs, model_path, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     94\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mmax_epochs,\n\u001b[0;32m     95\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m     ],\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    110\u001b[0m tft \u001b[38;5;241m=\u001b[39m TemporalFusionTransformer\u001b[38;5;241m.\u001b[39mfrom_dataset(\n\u001b[0;32m    111\u001b[0m     train,\n\u001b[0;32m    112\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.15\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     reduce_on_plateau_patience\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduce_on_plateau_patience\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m),\n\u001b[0;32m    121\u001b[0m )\n\u001b[1;32m--> 123\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Guardar el modelo entrenado\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_path:\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nicov\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[0;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[1;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[0;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "if grid_search == \"random\":\n",
    "    # Suprime todos los warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Llamada a la función de búsqueda aleatoria\n",
    "    best_model, best_params, best_val_loss = random_hyperparameter_search(\n",
    "        data,\n",
    "        train,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        test,\n",
    "        param_grid,\n",
    "        n_iterations=50,\n",
    "        max_epochs=epochs,\n",
    ",\n",
    "        save_dir=f'./plots/{target_file}_gauss_multiexog_-{n_prev_len}d-{date_start.replace(\"-\",\"\")}-{date_end.replace(\"-\",\"\")}',\n",
    "        csv_file=f\"./results/{target_file}_gauss_multiexog-{n_prev_len}d-{date_start}-{date_end}.csv\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e9473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search == \"exhaustive\":\n",
    "    \n",
    "    # Suprime todos los warnings \n",
    "\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Llamada a la función de búsqueda de hiperparámetros\n",
    "    best_model, best_params, best_val_loss = exhaustive_hyperparameter_search(\n",
    "        data, train, train_dataloader, val_dataloader, test, param_grid, max_epochs=1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
